# Submission information
submission:
  # Submission label
  # Label is used to index submissions, to avoid overlapping codes among submissions
  # use following way to form your label:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Shimada_SONY_task3b_2

  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2023 Audiovisual Microphone Array baseline

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight, maximum 10 characters
  abbreviation: MIC_AV_base

  # Submission authors in order, mark one of the authors as corresponding author.
  authors:
    # First author
    - lastname: Shimada
      firstname: Kazuki
      email: kazuki.shimada@sony.com                   # Contact email address
      corresponding: true                                 # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: SONY
        institute: SONY
        department:
        location: Tokyo, Japan
        
    # Second author
    - lastname: Politis
      firstname: Archontis
      email: archontis.politis@tuni.fi                  # Contact email address

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Audio Research Group
        location: Tampere, Finland


# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system. Use general level tags, if possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Model type (audio-only or audiovisual track)
    model_type: Audiovisual                    # Audio or Audiovisual

    # Audio input
    input_format: Microphone Array             # Ambisonic or Microphone Array or both
    input_sampling_rate: 24kHz                 #

    # Acoustic representation
    acoustic_features: magnitude spectra, IPD   # e.g one or multiple  [phase and magnitude spectra, mel spectra, GCC-PHAT, TDOA, intensity vector ...]
    # Video representation
    visual_features: video object likelihood    # e.g. one or multiple [raw video frames ...]


    # Data augmentation methods
    data_augmentation: !!null             # [time stretching, block mixing, pitch shifting, ...]

    # Machine learning
    # In case using ensemble methods, please specify all methods used (comma separated list).
    machine_learning_method: CRNN         # e.g one or multiple [GMM, HMM, SVM, kNN, MLP, CNN, RNN, CRNN, NMF, MHSA, random forest, ensemble, ...]
    
    #List external datasets in case of use for training
    external_datasets:  !!null              #AudioSet, ImageNet...

    #List here pre-trained models in case of use
    pre_trained_models: YOLOX               #AST, PANNs...


  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:

    # Total amount of parameters used in the acoustic model. For neural networks, this
    # information is usually given before training process in the network summary.
    # For other than neural networks, if parameter count information is not directly available,
    # try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    total_parameters: 500000

  # URL to the source code of the system [optional]
  source_code: https://github.com/sony/audio-visual-seld-dcase2023

# System results
results:

  development_dataset:
    # System result for development dataset with the provided evaluation setup.

    # Overall score 
    overall:
      ER_20: 0.71
      F_20: 18.0
      LE_CD: 32.2
      LR_CD: 47.0
