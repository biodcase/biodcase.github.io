# Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task7_track[track]_[index number of your submission (1-4)]
  label: Choi_GLI_task7_trackB_1

  # Submission name
  # This name will be used in the results tables when space permits.
  name: DCASE2023 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use a maximum of 10 characters.
  abbreviation: Baseline

  # Authors of the submitted system.
  # Mark authors in the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author, this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: Keunwoo
      firstname: Choi
      email: keunwoo@gaudiolab.com # Contact email address
      corresponding: true # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        institution: Gaudio Lab, Inc.
        department: AI Research # Optional
        location: Seoul, Korea

    # Second author
    - lastname: Jaekwon
      firstname: Im
      email: jaekwon@gaudiolab.com

      # Affiliation information for the author
      affiliation:
        institution: Gaudio Lab, Inc./Korea Advanced Institute of Science & Technology (KAIST)
        department: AI Research/Graduate School of Culture Technology # Optional
        location: Seoul, Korea/Daejeon, Korea

    # Third author
    - lastname: Laurie
      firstname: Heller
      email: laurieheller@cmu.edu

      # Affiliation information for the author
      affiliation:
        institution: Carnegie Mellon University
        department: Psychology # Optional
        location: Pittsburgh, US

# System results
results:
  # Google Colab URL to generate sounds for evaluation [mandatory]
  # The sounds must be unique and must be generated by the code supplied in the colab.
  colab_url: https://colab.research.google.com/drive/1FzbBf_FqWKu59i97ITibJbdPAqSzeMD4?usp=sharing

  development_dataset:
    # System results for development dataset
    # Full results are not mandatory, however, they are highly recommended as they are needed for a thorough analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete results can be reported.
    # Average FAD
    average:
      FAD: 9.702

    # Class-wise FAD
    class_wise:
      DogBark:
        FAD: 13.411
      Footstep:
        FAD: 8.109
      GunShot:
        FAD: 7.951
      Keyboard:
        FAD: 5.230
      MovingMotorVehicle:
        FAD: 16.108
      Rain:
        FAD: 13.337
      Sneeze/Cough:
        FAD: 3.770

  # URL to the source code of the system [optional]
  source_code: https://github.com/DCASE2023-Task7-Foley-Sound-Synthesis/dcase2023_task7_baseline


# System information
system:
  # System description, metadata provided here will be used to do a meta-analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:
    # System input
    # Please specify all system input used (comma-separated list).
    input: sound event label

    # Machine learning methods
    # In case using ensemble methods, please specify all methods used (comma-separated list).
    # e.g. AE, VAE, GAN, Transformer, diffusion model, ensemble...
    machine_learning_method: VQ-VAE, PixelSNAIL
    phase_reconstruction: HiFi-GAN

    # Generated acoustic feature input to phase reconstructor
    # One or multiple labels, e.g. MFCC, log-mel energies, spectrogram, CQT, ...
    acoustic_feature: spectrogram

    # System training/processing pipeline stages
    # e.g. "pretraining", "encoding" (from scratch), ,"weight quantization", "decoding", "phase reconstruction", ...
    pipeline: pretraining, encoding, weight quantization, decoding, phase reconstruction

    # Data augmentation methods
    # Please specify all methods used (comma-separated list).
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null    

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    # e.g. 2, 3, 4, 5, ...
    ensemble_method_subsystem_count: !!null

  # System complexity
  complexity:
    # Total amount of parameters used in the acoustic model(s) and phase reconstruction method(s).
    # For neural networks, this information is usually given before training process in the network summary.
    # For other than neural networks, if parameter count information is not directly available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding extraction networks and phase reconstruction methods.
    # Use numerical value.
    total_parameters: 269992

  # List of ALL external audio datasets used in the submission. (only for track A)
  # Development dataset is used here only as an example, list only external datasets
  # If multiple external audio datasets are used, please copy the lines after [# Dataset name] and list information on all the audio datasets.
  # e.g. AudioSet, ESC-50, URBAN-SED, Clotho, ...
  external_audio_datasets:
    # Dataset name
    - name: !!null

      # Dataset access URL
      url: !!null

      # Total audio length in minutes
      total_audio_length: !!null

  # List of ALL external pre-trained models used in the submission. (only for track A)
  # If multiple external pre-trained models are used, please copy the lines after [# Model name] and list information on all the pre-trained models.
  # e.g. PANNs, VGGish, AST, BYOL-A, ...
  external_models:
    # Model name
    - name: !!null

      # Access URL for pre-trained model
      url: !!null

      # How to use pre-trained model
      # e.g. encoder, decoder, weight quantization, vocoder, ... (comma-separated list)
      usage: !!null

  # URL to the source code of the system [optional, highly recommended]
  # Reproducibility will be used to evaluate submitted systems.
  source_code: https://github.com/DCASE2023-Task7-Foley-Sound-Synthesis/dcase2023_task7_baseline

# Questionnaire
questionnaire:
  # Do you agree to allow the DCASE distribution of 700 audio samples (100 samples * 7 audio categories) to evaluator(s) for the subjective evaluation? [mandatory]
  # The audio samples will not be distributed for any purpose other than subjective evaluation without other explicit permissions.
  distribute_audio_samples: Yes

  # Do you give permission to publish 700 audio samples (100 samples * 7 audio categories) used in the evaluation on the challenge result page?
  # This is very important from the perspective of reproducible research, and we strongly encourage you to allow it.
  # This does not mean that the copyright of audio samples is transferred to the DCASE community or task 7 organizers.
  publish_audio_samples: Yes

  # Do you agree to allow the DCASE use of 100 audio samples per category in a future version of this DCASE competition? (not required for competition entry, optional).
  # This may be used in future baseline comparisons or classification challenges related to this Foley challenge.
  # This does not mean that the copyright of audio samples is transferred to the DCASE community or task 7 organizers.
  use_audio_samples: Yes


