# Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task7_1(Index number of your submission. For task7, only 1 submission per team will be accepted.)
  label: Lee_GLI_task7_1

  # Submission name
  # This name will be used in the results tables when space permits.
  name: DCASE2024 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use a maximum of 10 characters.
  abbreviation: Baseline

  # Authors of the submitted system.
  # Mark authors in the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author, this will be listed next to the submission in the results tables.
  authors:
    # First author
    - firstname: Junwon
      lastname: Lee
      email: junwon.lee@gaudiolab.com

      # Affiliation information for the author
      affiliation:
        institution: Gaudio Lab, Inc./Korea Advanced Institute of Science & Technology (KAIST)
        department: AI Research/Music and Audio Computing Lab # Optional
        location: Seoul, Korea/Daejeon, Korea

    # Second author
    - firstname: Mathieu
      lastname: Lagrange
      email: mathieu.lagrange@ls2n.fr # Contact email address
      corresponding: true # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        institution: CNRS, Ecole Centrale Nantes, Nantes Université
        department: LS2N # Optional
        location: Nantes, France

    # Third author
    - firstname: Modan
      lastname: Tailleur
      email: modan.tailleur@ls2n.fr

      # Affiliation information for the author
      affiliation:
        institution: CNRS, Ecole Centrale Nantes, Nantes Université
        department: Signal, IMage et Son (SIMS) # Optional
        location: Nantes, France

# System results
results:
  # Google Colab URL to generate sounds for evaluation [mandatory]
  # The sounds must be unique and must be generated by the code supplied in the colab.
  colab_url: https://colab.research.google.com/drive/1g5e89nnJBENteb-qASJxazgvuD2D_0EQ

  development_dataset:
    # System results for development dataset
    FAD: 61.2761

    # If you are unable to provide FAD for development dataset, also FAD results for other dataset than deverlopment dataset can be reported.
    # If information field is not applicable to the system, use "!!null".
    FAD_for_other_dataset: 61.2761 # Optional

    # Audio dataset used for calculating FAD
    evaluation_audio_datasets:
      # Dataset name
      - name: DCASE2024 Challenge Task 7 Development Dataset

        # Dataset access URL
        url: https://zenodo.org/records/10869644

        # Total audio length in minutes
        total_audio_length: 100

# System information
system:
  # System description, metadata provided here will be used to do a meta-analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:
    # System input
    # Please specify all system input used (comma-separated list).
    input: text prompt

    # Machine learning methods
    # In case using ensemble methods, please specify all methods used (comma-separated list).
    # e.g. AE, VAE, GAN, Transformer, diffusion model, ensemble...
    machine_learning_method: VAE, CLAP, U-Net-based latent diffusion model
    phase_reconstruction: HiFi-GAN

    # Generated acoustic feature input to phase reconstructor
    # One or multiple labels, e.g. MFCC, log-mel energies, spectrogram, mel-spectrogram, CQT, ...
    acoustic_feature: mel-spectrogram

    # System training/processing pipeline stages
    # e.g. "contrastive language-audio pretraining", "encoding", "decoding", "phase reconstruction", ...
    pipeline: contrastive language-audio pretraining, encoding, decoding, phase reconstruction

    # Data augmentation methods
    # Please specify all methods used (comma-separated list).
    # e.g. mixup, time stretching, block mixing, pitch shifting, conditioning augmentation, ...
    data_augmentation: conditioning augmentation

    # Ensemble method subsystem count
    # In case ensemble method is not used, mark !!null.
    # e.g. 2, 3, 4, 5, ...
    ensemble_method_subsystem_count: !!null

  # System complexity
  complexity:
    # Total amount of parameters used in the acoustic model(s) and phase reconstruction method(s).
    # For neural networks, this information is usually given before training process in the network summary.
    # For other than neural networks, if parameter count information is not directly available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding extraction networks and phase reconstruction methods.
    # Use numerical value.
    total_parameters: 269992

  # List of ALL external audio datasets used in the submission. [mandatory]
  # Development dataset is used here only as an example, list only external datasets
  # If multiple external audio datasets are used, please copy the lines after [# Dataset name] and list information on all the audio datasets.
  # e.g. AudioSet, AudioCaps, Clotho, ...
  external_audio_datasets:
    # Dataset name
    - name: DCASE2024 Challenge Task 7 Development Dataset

      # Dataset access URL
      url: https://zenodo.org/records/10869644

      # Total audio length in minutes
      total_audio_length: 100

  # List of ALL external pre-trained models used in the submission.
  # If multiple external pre-trained models are used, please copy the lines after [# Model name] and list information on all the pre-trained models.
  # e.g. PANNs, VGGish, AST, BYOL-A, AudioLDM, ...
  external_models:
    # Model name
    - name: HiFi-GAN

      # Access URL for pre-trained model
      url: https://drive.google.com/drive/folders/1-eEYTB5Av9jNql0WGBlRoi-WH2J7bp5Y

      # How to use pre-trained model
      # e.g. encoder, decoder, weight quantization, vocoder, ... (comma-separated list)
      usage: vocoder

  # URL to the source code of the system [optional, highly recommended]
  # Reproducibility will be used to evaluate submitted systems.
  source_code: https://github.com/DCASE2024-Task7-Sound-Scene-Synthesis/AudioLDM-training-finetuning

# Questionnaire
questionnaire:
  # Do you agree to allow the DCASE distribution of 250 audio samples to evaluator(s) for the subjective evaluation? [mandatory]
  # The audio samples will not be distributed for any purpose other than subjective evaluation without other explicit permissions.
  distribute_audio_samples: Yes

  # Do you give permission for the task organizer to conduct a meta-analysis on 250 audio samples and to publish a technical report and paper using the results? [mandatory]
  # This does not mean that the copyright of audio samples is transferred to the DCASE community or task 7 organizers.
  publish_audio_samples: Yes

  # Do you agree to allow the DCASE use of 250 audio samples in a future version of this DCASE competition? (not required for competition entry, optional).
  # This may be used in future baseline comparisons or classification challenges related to this Foley challenge.
  # This does not mean that the copyright of audio samples is transferred to the DCASE community or task 7 organizers.
  use_audio_samples: Yes


