# Submission information for task 9
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid overlapping codes among submissions
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_submission_[index number of your submission (1-4)]  
  label: Liu_Surrey_task9_1

  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2024 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use maximum 10 characters.
  abbreviation: Baseline

# Authors of the submitted system. Mark authors in
# the order you want them to appear in submission lists.
# One of the authors has to be marked as corresponding author,
# this will be listed next to the submission in the results tables.
authors:
  # First author
  - lastname: Liu
    firstname: Xubo
    email: xubo.liu@surrey.ac.uk                # Contact email address
    corresponding: true                         # Mark true for one of the authors

    # Affiliation information for the author
    affiliation:
      abbreviation: Surrey
      institute: University of Surrey
      department: Centre for Vision, Speech and Signal Processing   # Optional
      location: Guilford, Surrey

  # Second author
  # ...

# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:
    # Audio input sampling rate
    # e.g., 16kHz, 32kHz
    input_sampling_rate: 16kHz

    # Input Acoustic representation
    # Here you should indicate which audio representation you used as system input. 
    input_acoustic_features: waveform

    # Data augmentation methods
    # e.g., volume augmentation
    data_augmentation: volume augmentation

    # Method scheme
    # Here you should indicate the scheme of the method that you used. For example
    machine_learning_method: CLAP, ResUNet-based separation model, time-frequency masking

    # Ensemble
    # - Here you should indicate the number of systems involved if you used ensembling.
    # - If you did not use ensembling, just write 1.
    ensemble_num_systems: 1

    # Loss function
    # - Here you should indicate the loss fuction that you employed.
    loss_function: waveform l1 loss

    # List of ALL pre-trained models used in the submission.
    # If multiple pre-trained models are used, please copy the lines after [# Model name] and list information on all the pre-trained models.
    # e.g. CLAP, AudioSep ...
    - name: CLAP
      # Access URL for pre-trained model
      url: https://github.com/LAION-AI/CLAP

      # How to use pre-trained model
      # e.g. text encoder, separation model
      usage: text encoder
  
  # Training configurations
  train_config:
    # Optimizer
    # - Here you should indicate the name of the optimizer that you used. 
    optimizer: AdamW
    # Learning rate
    # - Here you should indicate the learning rate of the optimizer that you used.
    learning_rate: 1e-4
    # Weight decay
    # - Here you should indicate if you used any weight decay of your optimizer.
    # - Be careful because most optimizers uses a non-zero value by default.
    # - Use 0 for no weight decay.
    weight_decay: 2
    # Gradient clipping
    # - Here you should indicate if you used any gradient clipping. 
    # - Use 0 for no clipping.
    gradient_clipping: 1
    # Gradient norm
    # - Here you should indicate the norm of the gradient that you used for gradient clipping.
    # - Use !!null for no clipping.
    gradient_norm: "L2"
    # Training steps of your entire system.
    # - WARNING: In case of ensembling, add up steps for all subsystems trained.
    steps: 200000
    # Number of GPUs used for training
    gpu_count: 1
    # Total number of batch size used for training
    batch_size: 16
    # GPU model name
    gpu_model: NVIDIA A100

  # submitted systems from the computational load perspective.
  complexity:
    # Learnable parameters
    learnable_parameters: 26.45M
    # Total amount of parameters involved at inference time
    total_parameters: 238.60M 
  
  # List of datasets used for training your system.
  # Unless you also used them to train your LASS system, you do not need to include datasets involved to your pre-trained modules (e.g., datasets used to train CLAP models).
  # If the audio clips have caption annotations, you should specify their type (e.g., text labels, human-annotated caption, machine-generated caption).
  train_datasets:
    - # Dataset name
      name: LASS Task9 Development (Clotho)
      # Audio source (use !!null if not applicable)
      source: Freesound
      # Dataset access url
      url: https://doi.org/10.5281/zenodo.3490683
      # Is private
      is_private: No
      # Has audio:
      has_audio: Yes
      # Has images
      has_images: No
      # Has video
      has_video: No
      # Has captions
      has_captions: Yes
      # Captions type
      captions_type: human-annotated caption
      # Number of captions per audio
      nb_captions_per_audio: 5
      # Total amount of examples used
      total_audio_length: 6972
      # Total duration of audio clips (hours)
      total_duration: 37
      # Used for (e.g., lass_modelling)
      used_for: lass_modelling

    - # Dataset name
      name: LASS Task9 Development (FSD50K)
      # Audio source (use !!null if not applicable)
      source: Freesound
      # Dataset access url
      url: https://zenodo.org/record/4060432
      # Is private
      is_private: No
      # Has audio:
      has_audio: Yes
      # Has images
      has_images: No
      # Has video
      has_video: No
      # Has captions
      has_captions: Yes
      # Captions type
      captions_type: machine-generated caption
      # Number of captions per audio
      nb_captions_per_audio: 1
      # Total amount of examples used
      total_audio_length: 51197
      # Total duration of audio clips (hours)
      total_duration: 108
      # Used for (e.g., lass_modelling)
      used_for: lass_modelling

  # URL to the source code of the system (optional, write !!null if you do not want to share code)
  source_code: https://github.com/Audio-AGI/dcase2024_task9_baseline

# System results
results:
  validation_results:
    # System results on the validation (synth) split.
    # - Full results are not mandatory, however, they are highly recommended as they are needed for thorough analysis of the challenge submissions.
    # - If you are unable to provide all the results, incomplete results can also be reported.
    # - Each score should contain at least 3 decimals.
    SDR: 5.708
    SDRi: 5.673
    SISDR: 3.862

# Additional question
additional_question:
  # Does the submitted system need to be manually checked? For example, generative model-based approachs (e.g., diffusion model) 
  # usually perform not well in SDR-based metrics. In this case, organizers will randomly select a few separated audio files on 
  # which they will check algorithms that obtained lower SDR results with informal listening tests, 
  # and at their discretion will decide to include them in the subjective evaluation.
  need_manual_check: no
  detailed_reason: "null"

# Questionnaire
questionnaire:
  # Do you agree to allow the DCASE distribution of 200 separated audio samples in evaluation (real) to evaluator(s) for the subjective evaluation? [mandatory]
  # The audio samples will not be distributed for any purpose other than subjective evaluation without other explicit permissions.
  distribute_audio_samples: Yes

  # Do you give permission for the task organizer to conduct a meta-analysis on your submitted audio samples and to publish a technical report and paper using the results? [mandatory]
  # This does not mean that the copyright of audio samples is transferred to the DCASE community or task 9 organizers.
  publish_audio_samples: Yes

  # Do you agree to allow the DCASE use of your submitted separated audio samples in a future version of this DCASE competition? (not required for competition entry, optional).
  # This may be used in future baseline comparisons or separation challenges.
  # This does not mean that the copyright of audio samples is transferred to the DCASE community or task 9 organizers.
  use_audio_samples: Yes
