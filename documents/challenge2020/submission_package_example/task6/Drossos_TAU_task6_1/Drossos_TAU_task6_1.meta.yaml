# Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid
  # overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Drossos_TAU_task6_1.meta.yaml

  # Submission name
  # This name will be used in the results tables when space permits
  name: DCASE2020 baseline system

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use maximum 10 characters.
  abbreviation: Baseline

  # Authors of the submitted system. Mark authors in
  # the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author,
  # this will be listed next to the submission in the results tables.
  authors:
    # First author
    - lastname: Drossos
      firstname: Konstantinos
      email: konstantinos.drossos@tuni.fi         # Contact email address
      corresponding: true                         # Mark true for one of the authors

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences            # Optional
        location: Tampere, Finland

    # Second author
    - lastname: Lipping
      firstname: Samuel
      email: samuel.lipping@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

    # Third author
    - lastname: Virtanen
      firstname: Tuomas
      email: tuomas.virtanen@tuni.fi

      # Affiliation information for the author
      affiliation:
        abbreviation: TAU
        institute: Tampere University
        department: Computing Sciences
        location: Tampere, Finland

# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input
    # e.g. 16kHz, 22.05kHz, 44.1kHz
    input_sampling_rate: 44.1kHz

    # Acoustic representation
    # one or multiple labels, e.g. MFCC, log-mel energies, spectrogram, CQT, raw waveform, ...
    acoustic_features: !!null

    # Word representation
    # one or multiple labels, e.g. embeddings, one-hot, ...
    word_features: !!null

    # Usage of available metadata (i.e. the tags of each file)
    # Either True or False
    used_metadata: False

    # Data augmentation methods
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: !!null

      # Machine learning
      # In case using ensemble methods, please specify all methods used (comma separated list).
    # one or multiple, e.g. seq2seq
    machine_learning_method: MLP

    # In the case that you used a seq2seq, indicate
    # what encoder you have, e.g. multi-layer GRU, CNNs, etc
    seq2seq_encoder: multi-layer RNN

    # and what decoder you have, e.g. multi-layer GRU, CNNs, etc
    seq2seq_decoder: multi-layer RNN

      # Classifier
    # e.g. feed-forward, SVM, ...
    classifier: feed-forward

  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:
    # Total amount of parameters used in the system.
    # For neural networks, this information is usually given before training process
    # in the network summary.
    # For other than neural networks, if parameter count information is not directly
    # available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding
    # extraction networks and classification network.
    # If you use extra modules during training, but not during testing, here
    # indicate the total (i.e. during training) amount of parameters.
    # Use numerical value.
    total_parameters: 5012931 # embeddings (OpenL2)=4684224, classifier=328707

    # And here, if different from `total_parameters`, indicate
    # the amount of parameters that your system use during inference.
    inference_parameters: 10931

  # URL to the source code of the system [optional, but strongly encouraged]
  source_code: https://github.com/audio-captioning/dcase-2020-baseline

  # URL to the pre-trained weights of the system [optional, but strongly encouraged]
  pre_trained_weights: https://zenodo.org/record/3697687

# System results
results:
  # For both development and evaluation results, full results are not mandatory,
  # however, they are highly recommended as they are needed for through analysis
  # of the challenge submissions. If you are unable to provide all results, also
  # incomplete results can be reported.
  #
  # For all metrics, numerical precision should be at three decimal digits.

  development_dataset:
    # System results for **development** dataset with provided setup.
    # Final results are reported at the **evaluation** section, just
    # below this one.

    # Metrics
    bleu_1: 0.389
    bleu_2: 0.136
    bleu_3: 0.055
    bleu_4: 0.015
    rouge_l: 0.262
    meteor: 0.084
    cider: 0.074
    spice: 0.033
    spider: 0.054

  evaluation_dataset:
    # System results for **evaluation** dataset with provided the evaluation setup.
    # These results are the ones that will rank your submission.

    # Metrics
    bleu_1:  # add here the BLEU 1 score of your method for DCASE evaluation split
    bleu_2:  # add here the BLEU 2 score of your method for DCASE evaluation split
    bleu_3:  # add here the BLEU 3 score of your method for DCASE evaluation split
    bleu_4:  # add here the BLEU 4 score of your method for DCASE evaluation split
    rouge_l: # add here the ROUGE L score of your method for DCASE evaluation split
    meteor:  # add here the METEOR score of your method for DCASE evaluation split
    cider:   # add here the CIDEr score of your method for DCASE evaluation split
    spice:   # add here the SPICE score of your method for DCASE evaluation split
    spider:  # add here the SPIDEr score of your method for DCASE evaluation split
