<!DOCTYPE html><html lang="en">
<head>
    <title>Technical program - BioDCASE</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Technical program - BioDCASE" />
    <link href="https://biodcase.github.io/favicon_.ico" rel="icon">
    <link rel="canonical" href="https://biodcase.github.io/workshop2017/technical-program">
    <meta property="og:url" content="https://dcase.community/workshop2017/technical-program" />
    <meta property="og:image" content="https://dcase.community/images/logos/default_link_preview2.png" />
    <meta name="author" content="Toni Heittola" />
    <meta name="description" content="Day 1 Thursday 16.11.2017, 9:00 - 18:00 Hours 8:45 Registration Registration 8:45 Coffee Coffee Welcome coffee 9:10 Welcome Welcome Annamaria Mesaros Tampere University of Technology, Finland 9:20 Keynote Keynote Session chair Sacha Krstulović General-Purpose Sound Event Recognition Shawn Hershey Google Research Slides Abstract …" />
    <meta name="og:description" content="Day 1 Thursday 16.11.2017, 9:00 - 18:00 Hours 8:45..." />

    <link href="https://fonts.googleapis.com/css?family=Heebo:900" rel="stylesheet">
    <link rel="stylesheet" href="https://biodcase.github.io/theme/assets/bootstrap/css/bootstrap.min.css" type="text/css"/>
    <link rel="stylesheet" href="https://biodcase.github.io/theme/assets/font-awesome/css/font-awesome.min.css" type="text/css"/>
    <link rel="stylesheet" href="https://biodcase.github.io/theme/assets/dcaseicons/css/dcaseicons.css?v=1.1" type="text/css"/>
        <link rel="stylesheet" href="https://biodcase.github.io/theme/assets/highlight/styles/monokai-sublime.css" type="text/css"/>
    <link rel="stylesheet" href="https://biodcase.github.io/theme/css/btoc.min.css">
    <link rel="stylesheet" href="https://biodcase.github.io/theme/css/theme.css?v=1.1" type="text/css"/>
    <link rel="stylesheet" href="https://biodcase.github.io/custom.css" type="text/css"/>
    <script type="text/javascript" src="https://biodcase.github.io/theme/assets/jquery/jquery.min.js"></script>
</head>
<body>
<nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation" data-spy="affix" data-offset-top="195">
    <div class="container">
        <div class="row">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <!-- <span class="icon-bar"></span> -->
                </button>
                <a href="https://biodcase.github.io/" class="navbar-brand hidden-lg hidden-md hidden-sm" ><img src="https://biodcase.github.io/images/logos/dcase/BioDCASE.png"/></a>
            </div>
            <div id="navbar-main" class="navbar-collapse collapse">                <!-- THIS IS THE NAV BAR (RIGHT SIDE) --><ul class="nav navbar-nav navbar-right" id="menuitem-list"><li class="" data-toggle="tooltip" data-placement="bottom" title="Google discussions for the BioDCASE Community">
        <a href="https://groups.google.com/g/biodcase-community" target="_blank" ><i class="fa fa-comments fa-1x fa-fw"></i>&nbsp;</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Github repository">
        <a href="https://github.com/biodcase" target="_blank" ><i class="fa fa-github fa-1x"></i>&nbsp;</a>
    </li>                </ul>            </div>
        </div><div class="row">
             <div id="navbar-sub" class="navbar-collapse collapse">
                <ul class="nav navbar-nav navbar-right " id="menuitem-list-sub"><li class="disabled subheader" >
        <a><strong>Workshop2017</strong></a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Workshop home">
        <a href="https://biodcase.github.io/workshop2017/"><i class="fa fa-home fa-fw"></i>&nbsp;Home</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Technical program">
        <a href="https://biodcase.github.io/workshop2017/technical-program"><i class="fa fa-list fa-fw"></i>&nbsp;Program</a>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Proceedings">
        <a href="https://biodcase.github.io/workshop2017/proceedings"><i class="fa fa-file fa-fw"></i>&nbsp;Proceedings</a>
    </li><li class="btn-group ">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown"><i class="fa fa-user fa-fw"></i>&nbsp;Authors&nbsp;<b class="caret"></b></a>
        <ul class="dropdown-menu" role="menu">
            <li class="">
        <a href="https://biodcase.github.io/workshop2017/author-instructions"><i class="fa fa-info fa-fw"></i>&nbsp;Instructions for Authors</a>
    </li>
            <li class="">
        <a href="https://biodcase.github.io/workshop2017/call-for-papers"><i class="fa fa-info fa-fw"></i>&nbsp;Call for papers</a>
    </li>
        </ul>
    </li><li class="" data-toggle="tooltip" data-placement="bottom" title="Organizing Committee">
        <a href="https://biodcase.github.io/workshop2017/organizers"><i class="fa fa-users fa-fw"></i>&nbsp;Organizers</a>
    </li></ul>
             </div>
        </div></div>
</nav>
<header class="page-top" style="background-image: url(../theme/images/everyday-patterns/metro-brussels-01.jpg);box-shadow: 0px 1000px rgba(120, 72, 0, 0.65) inset;overflow:hidden;position:relative">
    <div class="container" style="box-shadow: 0px 1000px rgba(255, 255, 255, 0.1) inset;;height:100%;padding-top:80px;padding-bottom:80px">
        <div class="row">
            <div class="col-lg-12 col-md-12">
                <div class="page-heading text-right"><div class="pull-right"><object class="img img-responsive sr-header-overlay" type="image/svg+xml" data="..//images/overlays/wave.svg"></object></div><h1 class="bold">Technical program</h1><span class="subheading">Workshop on Detection and Classification of Acoustic Scenes and Events</span><hr class="small right bold">
                        <span class="subheading subheading-secondary">16 - 17 November 2017, Munich, Germany</span></div>
            </div>
        </div>
    </div>
    <!-- <span class="header-cc-logo" data-toggle="tooltip" data-placement="top" title="Background photo by Toni Heittola / CC BY-NC 4.0"><i class="fa fa-creative-commons" aria-hidden="true"></i></span> -->
</header><div class="container">
    <div class="row">
        <div class="col-sm-3 sidecolumn-left ">
 <div class="btoc-container hidden-print" role="complementary">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title">Content</h3>
</div>
<ul class="nav btoc-nav">
<li><a href="#day-1">Day 1</a>
<ul>
<li><a href="#registration">Registration</a></li>
<li><a href="#coffee">Coffee</a></li>
<li><a href="#welcome">Welcome</a></li>
<li><a href="#keynote">Keynote</a></li>
<li><a href="#coffee-break">Coffee break</a></li>
<li><a href="#oral-session-i">Oral Session I</a></li>
<li><a href="#lunch">Lunch</a></li>
<li><a href="#oral-session-ii">Oral Session II</a></li>
<li><a href="#coffee-2">Coffee</a></li>
<li><a href="#poster-session-i">Poster Session I</a></li>
<li><a href="#dcase2017-challenge-posters">DCASE2017 Challenge posters</a></li>
<li><a href="#open-discussion">Open Discussion</a></li>
</ul>
</li>
<li><a href="#day-2">Day 2</a>
<ul>
<li><a href="#keynote-1">Keynote</a></li>
<li><a href="#oral-session-iii">Oral Session III</a></li>
<li><a href="#coffee-3">Coffee</a></li>
<li><a href="#poster-session-ii">Poster Session II</a></li>
<li><a href="#closing-remarks">Closing remarks</a></li>
</ul>
</li></ul></div>
</div>

        </div>
        <div class="col-sm-9 content">
            <section id="content" class="body">
                <div class="entry-content">
                    <h2 id="day-1">Day 1</h2>
<p><small><i aria-hidden="true" class="fa fa-clock-o"></i> Thursday 16.11.2017, 9:00 - 18:00</small></p>
<table class="table table-condensed schedule">
<thead>
<tr>
<th style="width:50px;">Hours</th>
<th style="width:100px;"></th>
<th></th>
</tr>
</thead>
<tbodv>
<tr>
<td>8:45</td>
<td class="active">Registration</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;">
<div class="row" style="margin-top:10px;">
<div class="col-xs-5"></div>
</div>
</td>
<td style="border-top: none;padding-top:0px;">
<h3 id="registration">Registration</h3>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>8:45</td>
<td class="active">Coffee</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;"><h3 id="coffee">Coffee</h3>
<p>Welcome coffee</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>9:10</td>
<td class="success">Welcome</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;">
<div class="row" style="margin-top:10px;">
<div class="col-xs-5"></div>
</div>
</td>
<td style="border-top: none;padding-top:0px;">
<h3 id="welcome">Welcome</h3>
<p>Annamaria Mesaros <br/><span class="text-muted"><em>Tampere University of Technology, Finland</em></span></p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>9:20</td>
<td class="danger">Keynote</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;">
<div class="row" style="margin-top:10px;">
<div class="col-xs-5"></div>
</div>
</td>
<td style="border-top: none;padding-top:0px;">
<h3 id="keynote">Keynote</h3>
<p class="text-muted"><em>Session chair Sacha Krstulović<em></em></em></p>
<div class="btex-item" data-item="Hershey_KEYNOTE" data-source="content/data/workshop2017/extra.bib" data-template="fancy_minimal_keynote">
<div class="row">
<div class="col-md-9">
<h4>General-Purpose Sound Event Recognition</h4><a name="Hershey_KEYNOTE"></a>
<p>
                    Shawn Hershey<br/>
<span class="text-muted"><small><em>Google Research</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="https://dcase.community/documents/workshop2017/presentations/the_story_of_audioset.pdf" rel="tooltip" title="Slides"><i class="fa fa-picture-o fa-1x"></i> Slides</a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=hthlT7MHtxM&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseHershey_KEYNOTEc7878ccb936f4458870f6f57b36cbca8" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseHershey_KEYNOTEc7878ccb936f4458870f6f57b36cbca8" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingHershey_KEYNOTEc7878ccb936f4458870f6f57b36cbca8" class="panel-collapse collapse" id="collapseHershey_KEYNOTEc7878ccb936f4458870f6f57b36cbca8" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Inspired by the success of general-purpose object recognition in images, we have been working on automatic, real-time systems for recognizing sound events regardless of domain. Our goal is a system that can tag or describe an arbitrary soundtrack - as might be found on a media sharing site like YouTube - using terms that make sense to a human. I will cover the process of defining this task, our deep learning approach, our efforts to collect training data, and our current results. I'll discuss some factors important for accurate models, and some ideas about how to get the best return from manual labeling investment.</p>
<h5>Biography</h5>
<p class="text-justify">Shawn Hershey is a software engineer at Google Research, working in the Machine Hearing Group on machine learning for speech and audio processing. He is currently working on soundtrack classification and audio event detection. Before Google he worked as the first Software Engineer at Lyric Semiconductors, building tools to aid the development of hardware accelerators for AI. On the side, Shawn travels the world teaching Lindy Hop and blues dancing and playing in swing and blues bands. Long ago Shawn graduated from the University of Rochester with a BA in Computer Science and half of a degree from the Eastman School of Music.</p>
<div class="row">
<div class="col-md-10">
<h5><strong>Shawn Hershey</strong></h5>
<p><em>
                    
                        Google Research
                              
                    </em></p>
</div>
<div class="col-md-2">
<img class="img img-rounded" src="http://dcase.community/images/person/shawn_hershey.jpg"/>
</div>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="https://dcase.community/documents/workshop2017/presentations/the_story_of_audioset.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o"></i> Slides</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=hthlT7MHtxM&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>10:10</td>
<td class="active">Break</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;"><h3 id="coffee-break">Coffee break</h3></td>
</tr>
</table>
</td>
</tr>
<tr>
<td>10:30</td>
<td class="success">Presentations</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;">
<h3 id="oral-session-i">Oral Session I</h3>
<p class="text-muted"><em>Session chair Axel Plinge<em></em></em></p>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">10:30</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="DCASE2017_SUMMARY" data-source="content/data/workshop2017/extra.bib" data-template="fancy_minimal_no_bibtex">
<div class="row">
<div class="col-md-9">
<h4>DCASE2017 Challenge Summary</h4><a name="DCASE2017_SUMMARY"></a>
<p>
                    Tuomas Virtanen<br/>
<span class="text-muted"><small><em>Tampere University of Technology, Laboratory of Signal Processing, Tampere, Finland</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=SSPZ1z80Ikc" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
</div>
</div>
</div>
<div aria-labelledby="headingDCASE2017_SUMMARY7d1388d655d9448692c594d7475ea37e" class="panel-collapse collapse" id="collapseDCASE2017_SUMMARY7d1388d655d9448692c594d7475ea37e" role="tabpanel">
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=SSPZ1z80Ikc" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
<div class="btex-item" data-item="Mesaros2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>DCASE2017 Challenge Setup: Tasks, Datasets and Baseline System</h4><a name="Mesaros2017"></a>
<p>
                    Annamaria Mesaros<sup>1</sup>, Toni Heittola<sup>1</sup>, Aleksandr Diment<sup>1</sup>, Benjamin Elizalde<sup>2</sup>, Ankit Shah<sup>2</sup>, Emmanuel Vincent<sup>3</sup>, Bhiksha Raj<sup>2</sup> and Tuomas Virtanen <sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Tampere University of Technology, Laboratory of Signal Processing, Tampere, Finland, <sup>2</sup>Carnegie Mellon University, Department of Electrical and Computer Engineering, &amp; Department of Language Technologies Institute, Pittsburgh, USA, <sup>3</sup>Inria, F-54600 Villers-les-Nancy, France</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mesaros_100.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapseMesaros20172d26454b84d34d5da2236ab89c209214" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros20172d26454b84d34d5da2236ab89c209214" type="button">
<i class="fa fa-git"></i>
</button>
<button aria-controls="collapseMesaros20172d26454b84d34d5da2236ab89c209214" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMesaros20172d26454b84d34d5da2236ab89c209214" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMesaros20172d26454b84d34d5da2236ab89c209214" class="panel-collapse collapse" id="collapseMesaros20172d26454b84d34d5da2236ab89c209214" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events, Weak Labels</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMesaros20172d26454b84d34d5da2236ab89c209214" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mesaros_100.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success" href="https://github.com/TUT-ARG/DCASE2017-baseline-system" style="text-decoration:none;border-bottom:0;padding-bottom:9px" title=""><i class="fa fa-git"></i> </a>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMesaros20172d26454b84d34d5da2236ab89c209214label" class="modal fade" id="bibtexMesaros20172d26454b84d34d5da2236ab89c209214" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMesaros20172d26454b84d34d5da2236ab89c209214label">DCASE2017 Challenge Setup: Tasks, Datasets and Baseline System</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mesaros2017,
    author = "Mesaros, Annamaria and Heittola, Toni and Diment, Aleksandr and Elizalde, Benjamin and Shah, Ankit and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas",
    title = "{DCASE2017} Challenge Setup: Tasks, Datasets and Baseline System",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "85--92",
    keywords = "Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events, Weak Labels",
    abstract = "DCASE 2017 Challenge consists of four tasks: acoustic scene classification, detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">11:00</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Mun2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using SVM Hyper-Plane</h4><a name="Mun2017"></a>
<p>
                    Seongkyu Mun<sup>1</sup>, Sangwook Park<sup>1</sup>, David Han<sup>2</sup> and Hanseok Ko<sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Intelligent Signal Processing Laboratory, Korea University, Seoul, South Korea, <sup>2</sup>Office of Naval Research, Office of Naval Research, Arlington VA, USA</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mun_215.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapseMun201791efff06e68e401aafed530102865998" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMun201791efff06e68e401aafed530102865998" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMun201791efff06e68e401aafed530102865998" class="panel-collapse collapse" id="collapseMun201791efff06e68e401aafed530102865998" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Although it is typically expected that using a large amount of labeled training data would lead to improve performance in deep learning, it is generally difficult to obtain such DataBase (DB). In competitions such as the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge Task 1, participants are constrained to use a relatively small DB as a rule, which is similar to the aforementioned issue. To improve Acoustic Scene Classification (ASC) performance without employing additional DB, this paper proposes to use Generative Adversarial Networks (GAN) based method for generating additional training DB. Since it is not clear whether every sample generated by GAN would have equal impact in classification performance, this paper proposes to use Support Vector Machine (SVM) hyper plane for each class as reference for selecting samples, which have class discriminative information. Based on the crossvalidated experiments on development DB, the usage of the generated features could improve ASC performance.</p>
<h5>Keywords</h5>
<p class="text-justify">acoustic scene classification, generative adversarial networks, support vector machine, data augmentation, decision hyper-plane</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexMun201791efff06e68e401aafed530102865998" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Mun_215.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexMun201791efff06e68e401aafed530102865998label" class="modal fade" id="bibtexMun201791efff06e68e401aafed530102865998" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexMun201791efff06e68e401aafed530102865998label">Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using SVM Hyper-Plane</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Mun2017,
    author = "Mun, Seongkyu and Park, Sangwook and Han, David K and Ko, Hanseok",
    title = "Generative Adversarial Network Based Acoustic Scene Training Set Augmentation and Selection Using {SVM} Hyper-Plane",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "93--102",
    keywords = "acoustic scene classification, generative adversarial networks, support vector machine, data augmentation, decision hyper-plane",
    abstract = "Although it is typically expected that using a large amount of labeled training data would lead to improve performance in deep learning, it is generally difficult to obtain such DataBase (DB). In competitions such as the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge Task 1, participants are constrained to use a relatively small DB as a rule, which is similar to the aforementioned issue. To improve Acoustic Scene Classification (ASC) performance without employing additional DB, this paper proposes to use Generative Adversarial Networks (GAN) based method for generating additional training DB. Since it is not clear whether every sample generated by GAN would have equal impact in classification performance, this paper proposes to use Support Vector Machine (SVM) hyper plane for each class as reference for selecting samples, which have class discriminative information. Based on the crossvalidated experiments on development DB, the usage of the generated features could improve ASC performance."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">11:20</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Lee2017b" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Ensemble of Convolutional Neural Networks for Weakly-supervised Sound Event Detection Using Multiple Scale Input</h4><a name="Lee2017b"></a>
<p>
                    Donmoon Lee<sup>1,2</sup>, Subin Lee<sup>1,2</sup>, Yoonchang Han<sup>2</sup> and Kyogu Lee<sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Music and Audio Research Group, Seoul National University, Seoul, Korea, <sup>2</sup>Cochlear.ai, Seoul, Korea</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lee_200.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Lee_200_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o fa-1x"></i></a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=HX5pQIqZ81A&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseLee2017b683e18a6514843d9a2421a12bc681509" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseLee2017b683e18a6514843d9a2421a12bc681509" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingLee2017b683e18a6514843d9a2421a12bc681509" class="panel-collapse collapse" id="collapseLee2017b683e18a6514843d9a2421a12bc681509" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">In this paper, we use ensemble of convolutional neural network models that use the various analysis window to detect audio events in the automotive environment. When detecting the presence of audio events, global input based model that uses the entire audio clip works better. On the other hand, segmented input based models works better in finding the accurate position of the event. Experimental results for weakly-labeled audio data confirm the performance trade-off between the two tasks, depending on the length of input audio. By combining the predictions of various models, the proposed system achieved 0.4762 in the clip-based F1-score and 0.7167 in the segment-based error rate.</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexLee2017b683e18a6514843d9a2421a12bc681509" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lee_200.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Lee_200_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o"></i> Slides</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=HX5pQIqZ81A&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexLee2017b683e18a6514843d9a2421a12bc681509label" class="modal fade" id="bibtexLee2017b683e18a6514843d9a2421a12bc681509" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexLee2017b683e18a6514843d9a2421a12bc681509label">Ensemble of Convolutional Neural Networks for Weakly-supervised Sound Event Detection Using Multiple Scale Input</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lee2017b,
    author = "Lee, Donmoon and Lee, Subin and Han, Yoonchang and Lee, Kyogu",
    title = "Ensemble of Convolutional Neural Networks for Weakly-supervised Sound Event Detection Using Multiple Scale Input",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    pages = "74--79",
    abstract = "In this paper, we use ensemble of convolutional neural network models that use the various analysis window to detect audio events in the automotive environment. When detecting the presence of audio events, global input based model that uses the entire audio clip works better. On the other hand, segmented input based models works better in finding the accurate position of the event. Experimental results for weakly-labeled audio data confirm the performance trade-off between the two tasks, depending on the length of input audio. By combining the predictions of various models, the proposed system achieved 0.4762 in the clip-based F1-score and 0.7167 in the segment-based error rate."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">11:40</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Adavanne2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Sound Event Detection Using Weakly Labeled Dataset with Stacked Convolutional and Recurrent Neural Network</h4><a name="Adavanne2017"></a>
<p>
                    Sharath Adavanne and Tuomas Virtanen<br/>
<span class="text-muted"><small><em>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Adavanne_129.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Adavanne_129_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o fa-1x"></i></a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=UDdkecyKDOc&amp;index=11&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseAdavanne2017cefaaf00db79494cb3018a13e6461c2b" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseAdavanne2017cefaaf00db79494cb3018a13e6461c2b" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingAdavanne2017cefaaf00db79494cb3018a13e6461c2b" class="panel-collapse collapse" id="collapseAdavanne2017cefaaf00db79494cb3018a13e6461c2b" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This paper proposes a neural network architecture and training scheme to learn the start and end time of sound events (strong labels) in an audio recording given just the list of sound events existing in the audio without time information (weak labels). We achieve this by using a stacked convolutional and recurrent neural network with two prediction layers in sequence one for the strong followed by the weak label. The network is trained using frame-wise log melband energy as the input audio feature, and weak labels provided in the dataset as labels for the weak label prediction layer. Strong labels are generated by replicating the weak labels as many number of times as the frames in the input audio feature, and used for strong label layer during training. We propose to control what the network learns from the weak and strong labels by different weighting for the loss computed in the two prediction layers. The proposed method is evaluated on a publicly available dataset of 155 hours with 17 sound event classes. The method achieves the best error rate of 0.84 for strong labels and F-score of 43.3% for weak labels on the unseen test split.</p>
<h5>Keywords</h5>
<p class="text-justify">sound event detection, weak labels, deep neural network, CNN, GRU</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexAdavanne2017cefaaf00db79494cb3018a13e6461c2b" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Adavanne_129.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Adavanne_129_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o"></i> Slides</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=UDdkecyKDOc&amp;index=11&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexAdavanne2017cefaaf00db79494cb3018a13e6461c2blabel" class="modal fade" id="bibtexAdavanne2017cefaaf00db79494cb3018a13e6461c2b" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexAdavanne2017cefaaf00db79494cb3018a13e6461c2blabel">Sound Event Detection Using Weakly Labeled Dataset with Stacked Convolutional and Recurrent Neural Network</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Adavanne2017,
    author = "Adavanne, Sharath and Virtanen, Tuomas",
    title = "Sound Event Detection Using Weakly Labeled Dataset with Stacked Convolutional and Recurrent Neural Network",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "12--16",
    keywords = "sound event detection, weak labels, deep neural network, CNN, GRU",
    abstract = "This paper proposes a neural network architecture and training scheme to learn the start and end time of sound events (strong labels) in an audio recording given just the list of sound events existing in the audio without time information (weak labels). We achieve this by using a stacked convolutional and recurrent neural network with two prediction layers in sequence one for the strong followed by the weak label. The network is trained using frame-wise log melband energy as the input audio feature, and weak labels provided in the dataset as labels for the weak label prediction layer. Strong labels are generated by replicating the weak labels as many number of times as the frames in the input audio feature, and used for strong label layer during training. We propose to control what the network learns from the weak and strong labels by different weighting for the loss computed in the two prediction layers. The proposed method is evaluated on a publicly available dataset of 155 hours with 17 sound event classes. The method achieves the best error rate of 0.84 for strong labels and F-score of 43.3\% for weak labels on the unseen test split."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">12:00</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Kroos2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Neuroevolution for Sound Event Detection in Real Life Audio: A Pilot Study</h4><a name="Kroos2017"></a>
<p>
                    Christian Kroos and Mark D. Plumbley<br/>
<span class="text-muted"><small><em>Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Kroos_191.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Kroos_191_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o fa-1x"></i></a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=3_qDZlLtCoY&amp;index=4&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseKroos2017c7af7ce82cb9403a800ac1a646d92740" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseKroos2017c7af7ce82cb9403a800ac1a646d92740" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingKroos2017c7af7ce82cb9403a800ac1a646d92740" class="panel-collapse collapse" id="collapseKroos2017c7af7ce82cb9403a800ac1a646d92740" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Neuroevolution techniques combine genetic algorithms with artificial neural networks, some of them evolving network topology along with the network weights. One of these latter techniques is the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. For this pilot study we devised an extended variant (joint NEAT, J-NEAT), introducing dynamic cooperative co-evolution, and applied it to sound event detection in real life audio (Task 3) in the DCASE 2017 challenge. Our research question was whether small networks could be evolved that would be able to compete with the much larger networks now typical for classification and detection tasks. We used the wavelet-based deep scattering transform and k-means clustering across the resulting scales (not across samples) to provide J-NEAT with a compact representation of the acoustic input. The results show that for the development data set J-NEAT was capable of evolving small networks that match the performance of the baseline system in terms of the segment-based error metrics, while exhibiting a substantially better event-related error rate. In the challenge, J-NEAT took first place overall according to the F1 error metric with an F1 of 44:9% and achieved rank 15 out of 34 on the ER error metric with a value of 0:891. We discuss the question of evolving versus learning for supervised tasks.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound event detection, neuroevolution, NEAT, deep scattering transform, wavelets, clustering, co-evolution</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexKroos2017c7af7ce82cb9403a800ac1a646d92740" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Kroos_191.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Kroos_191_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o"></i> Slides</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=3_qDZlLtCoY&amp;index=4&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexKroos2017c7af7ce82cb9403a800ac1a646d92740label" class="modal fade" id="bibtexKroos2017c7af7ce82cb9403a800ac1a646d92740" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexKroos2017c7af7ce82cb9403a800ac1a646d92740label">Neuroevolution for Sound Event Detection in Real Life Audio: A Pilot Study</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Kroos2017,
    author = "Kroos, Christian and Plumbley, Mark D.",
    title = "Neuroevolution for Sound Event Detection in Real Life Audio: A Pilot Study",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "64--68",
    keywords = "Sound event detection, neuroevolution, NEAT, deep scattering transform, wavelets, clustering, co-evolution",
    abstract = "Neuroevolution techniques combine genetic algorithms with artificial neural networks, some of them evolving network topology along with the network weights. One of these latter techniques is the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. For this pilot study we devised an extended variant (joint NEAT, J-NEAT), introducing dynamic cooperative co-evolution, and applied it to sound event detection in real life audio (Task 3) in the DCASE 2017 challenge. Our research question was whether small networks could be evolved that would be able to compete with the much larger networks now typical for classification and detection tasks. We used the wavelet-based deep scattering transform and k-means clustering across the resulting scales (not across samples) to provide J-NEAT with a compact representation of the acoustic input. The results show that for the development data set J-NEAT was capable of evolving small networks that match the performance of the baseline system in terms of the segment-based error metrics, while exhibiting a substantially better event-related error rate. In the challenge, J-NEAT took first place overall according to the F1 error metric with an F1 of 44:9\% and achieved rank 15 out of 34 on the ER error metric with a value of 0:891. We discuss the question of evolving versus learning for supervised tasks."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>12:30</td>
<td class="active">Break</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;"><h3 id="lunch">Lunch</h3></td>
</tr>
</table>
</td>
</tr>
<tr>
<td>14:00</td>
<td class="success">Presentations</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;">
<h3 id="oral-session-ii">Oral Session II</h3>
<p class="text-muted"><em>Session chair Romain Serizel<em></em></em></p>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">14:00</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Fonseca2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks</h4><a name="Fonseca2017"></a>
<p>
                    Eduardo Fonseca, Rong Gong, Dmitry Bogdanov, Olga Slizovskaia, Emilia Gomez and Xavier Serra<br/>
<span class="text-muted"><small><em>Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Fonseca_181.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Fonseca_181_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o fa-1x"></i></a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=0IgJQHE68AM&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseFonseca2017d8e388be430446a0b5cae7599566c977" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseFonseca2017d8e388be430446a0b5cae7599566c977" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingFonseca2017d8e388be430446a0b5cae7599566c977" class="panel-collapse collapse" id="collapseFonseca2017d8e388be430446a0b5cae7599566c977" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This work describes our contribution to the acoustic scene classification task of the DCASE 2017 challenge. We propose a system that consists of the ensemble of two methods of different nature: a feature engineering approach, where a collection of hand-crafted features is input to a Gradient Boosting Machine, and another approach based on learning representations from data, where log-scaled melspectrograms are input to a Convolutional Neural Network. This CNN is designed with multiple filter shapes in the first layer. We use a simple late fusion strategy to combine both methods. We report classification accuracy of each method alone and the ensemble system on the provided cross-validation setup of TUT Acoustic Scenes 2017 dataset. The proposed system outperforms each of its component methods and improves the provided baseline system by 8.2%.</p>
<h5>Keywords</h5>
<p class="text-justify">acoustic scene classification, gradient boosting machine, convolutional neural networks, ensembling</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexFonseca2017d8e388be430446a0b5cae7599566c977" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Fonseca_181.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Fonseca_181_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o"></i> Slides</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=0IgJQHE68AM&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexFonseca2017d8e388be430446a0b5cae7599566c977label" class="modal fade" id="bibtexFonseca2017d8e388be430446a0b5cae7599566c977" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexFonseca2017d8e388be430446a0b5cae7599566c977label">Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Fonseca2017,
    author = "Fonseca, Eduardo and Gong, Rong and Bogdanov, Dmitry and Slizovskaia, Olga and Gomez, Emilia and Serra, Xavier",
    title = "Acoustic Scene Classification by Ensembling Gradient Boosting Machine and Convolutional Neural Networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "37--41",
    keywords = "acoustic scene classification, gradient boosting machine, convolutional neural networks, ensembling",
    abstract = "This work describes our contribution to the acoustic scene classification task of the DCASE 2017 challenge. We propose a system that consists of the ensemble of two methods of different nature: a feature engineering approach, where a collection of hand-crafted features is input to a Gradient Boosting Machine, and another approach based on learning representations from data, where log-scaled melspectrograms are input to a Convolutional Neural Network. This CNN is designed with multiple filter shapes in the first layer. We use a simple late fusion strategy to combine both methods. We report classification accuracy of each method alone and the ensemble system on the provided cross-validation setup of TUT Acoustic Scenes 2017 dataset. The proposed system outperforms each of its component methods and improves the provided baseline system by 8.2\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">14:20</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Jimenez2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features</h4><a name="Jimenez2017"></a>
<p>
                    Abelino Jimenez, Benjamin Elizalde and Bhiksha Raj<br/>
<span class="text-muted"><small><em>Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, USA</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jimenez_195.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Jimenez_195_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o fa-1x"></i></a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=h1h625_vF0s&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseJimenez20171630d4eb7b104f14813b2dd79bd95534" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseJimenez20171630d4eb7b104f14813b2dd79bd95534" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingJimenez20171630d4eb7b104f14813b2dd79bd95534" class="panel-collapse collapse" id="collapseJimenez20171630d4eb7b104f14813b2dd79bd95534" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Acoustic scene recordings are represented by different types of handcrafted or Neural Network features. These features, typically of thousands of dimensions, are classified in state of the art approaches using kernel machines, such as the Support Vector Machines (SVM). However, the complexity of training these methods increases with the dimensionality of these input features and the size of the dataset. A solution is to map the input features to a randomized low-dimensional feature space. The resulting random features can approximate non-linear kernels with faster linear kernel computation. In this work, we computed a set of 6,553 input features and used them to compute random features to approximate three types of kernels, Guassian, Laplacian and Cauchy. We compared their performance using an SVM in the context of the DCASE Task 1 - Acoustic Scene Classification. Experiments show that both, input and random features outperformed the DCASE baseline by an absolute 4%. Moreover, the random features reduced the dimensionality of the input by more than three times with minimal loss of performance and by more than six times and still outperformed the baseline. Hence, random features could be employed by state of the art approaches to compute low-storage features and perform faster kernel computations.</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustic Scene Classification, Laplacian Kernel, Kernel Machines, Random Features</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexJimenez20171630d4eb7b104f14813b2dd79bd95534" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jimenez_195.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Jimenez_195_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o"></i> Slides</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=h1h625_vF0s&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexJimenez20171630d4eb7b104f14813b2dd79bd95534label" class="modal fade" id="bibtexJimenez20171630d4eb7b104f14813b2dd79bd95534" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexJimenez20171630d4eb7b104f14813b2dd79bd95534label">DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jimenez2017,
    author = "Jimenez, Abelino and Elizalde, Benjamin and Raj, Bhiksha",
    title = "{DCASE} 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "55--58",
    keywords = "Acoustic Scene Classification, Laplacian Kernel, Kernel Machines, Random Features",
    abstract = "Acoustic scene recordings are represented by different types of handcrafted or Neural Network features. These features, typically of thousands of dimensions, are classified in state of the art approaches using kernel machines, such as the Support Vector Machines (SVM). However, the complexity of training these methods increases with the dimensionality of these input features and the size of the dataset. A solution is to map the input features to a randomized low-dimensional feature space. The resulting random features can approximate non-linear kernels with faster linear kernel computation. In this work, we computed a set of 6,553 input features and used them to compute random features to approximate three types of kernels, Guassian, Laplacian and Cauchy. We compared their performance using an SVM in the context of the DCASE Task 1 - Acoustic Scene Classification. Experiments show that both, input and random features outperformed the DCASE baseline by an absolute 4\%. Moreover, the random features reduced the dimensionality of the input by more than three times with minimal loss of performance and by more than six times and still outperformed the baseline. Hence, random features could be employed by state of the art approaches to compute low-storage features and perform faster kernel computations."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">14:40</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Bisot2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Nonnegative Feature Learning Methods for Acoustic Scene Classification</h4><a name="Bisot2017"></a>
<p>
                    Victor Bisot<sup>1</sup>, Romain Serizel<sup>2,3,4</sup>, Slim Essid<sup>1</sup> and Gaël Richard<sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Image Data and Signal, Telecom ParisTech, Paris, France, <sup>2</sup>Université de Lorraine, Loria, Nancy, France, <sup>3</sup>Inria, Nancy, France, <sup>4</sup>CNRS, LORIA, Nancy, France</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Bisot_194.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=vzjRe0I3o8I&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseBisot2017a25ec8f43e7940cca2f7f99d6528942e" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseBisot2017a25ec8f43e7940cca2f7f99d6528942e" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingBisot2017a25ec8f43e7940cca2f7f99d6528942e" class="panel-collapse collapse" id="collapseBisot2017a25ec8f43e7940cca2f7f99d6528942e" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This paper introduces improvements to nonnegative feature learning-based methods for acoustic scene classification. We start by introducing modifications to the task-driven nonnegative matrix factorization algorithm. The proposed adapted scaling algorithm improves the generalization capability of task-driven nonnegative matrix factorization for the task. We then propose to exploit simple deep neural network architecture to classify both low level time-frequency representations and unsupervised nonnegative matrix factorization activation features independently. Moreover, we also propose a deep neural network architecture that exploits jointly unsupervised nonnegative matrix factorization activation features and low-level time frequency representations as inputs. Finally, we present a fusion of proposed systems in order to further improve performance. The resulting systems are our submission for the task 1 of the DCASE 2017 challenge.</p>
<h5>Keywords</h5>
<p class="text-justify">Feature learning, Nonnegative Matrix Factorization, Deep Neural Networks</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexBisot2017a25ec8f43e7940cca2f7f99d6528942e" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Bisot_194.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=vzjRe0I3o8I&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexBisot2017a25ec8f43e7940cca2f7f99d6528942elabel" class="modal fade" id="bibtexBisot2017a25ec8f43e7940cca2f7f99d6528942e" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexBisot2017a25ec8f43e7940cca2f7f99d6528942elabel">Nonnegative Feature Learning Methods for Acoustic Scene Classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Bisot2017,
    author = "Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gaël",
    title = "Nonnegative Feature Learning Methods for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "22--26",
    keywords = "Feature learning, Nonnegative Matrix Factorization, Deep Neural Networks",
    abstract = "This paper introduces improvements to nonnegative feature learning-based methods for acoustic scene classification. We start by introducing modifications to the task-driven nonnegative matrix factorization algorithm. The proposed adapted scaling algorithm improves the generalization capability of task-driven nonnegative matrix factorization for the task. We then propose to exploit simple deep neural network architecture to classify both low level time-frequency representations and unsupervised nonnegative matrix factorization activation features independently. Moreover, we also propose a deep neural network architecture that exploits jointly unsupervised nonnegative matrix factorization activation features and low-level time frequency representations as inputs. Finally, we present a fusion of proposed systems in order to further improve performance. The resulting systems are our submission for the task 1 of the DCASE 2017 challenge."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">15:00</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Amiriparian2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Sequence to Sequence Autoencoders for Unsupervised Representation Learning from Audio</h4><a name="Amiriparian2017"></a>
<p>
                    Shahin Amiriparian<sup>1,2,3</sup>, Michael Freitag<sup>1</sup>, Nicholas Cummins<sup>1,2</sup> and Björn Schuller<sup>2,4</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Chair of Complex &amp; Intelligent Systems, Universität Passau, Passau, Germany, <sup>2</sup>Chair of Embedded Intelligence for Health Care, Augsburg University, Augsburg, Germany, <sup>3</sup>Machine Intelligence &amp; Signal Processing Group, Technische Universität München, München, Germany, <sup>4</sup>Group of Language, Audio &amp; Music, Imperial Collage London, London, UK</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Amiriparian_172.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Amiriparian_172_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o fa-1x"></i></a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=mcgQWSNaFkI&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseAmiriparian20178ed4f008257e49f2b31719f343bd83ad" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseAmiriparian20178ed4f008257e49f2b31719f343bd83ad" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingAmiriparian20178ed4f008257e49f2b31719f343bd83ad" class="panel-collapse collapse" id="collapseAmiriparian20178ed4f008257e49f2b31719f343bd83ad" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This paper describes our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). We propose a system for this task using a recurrent sequence to sequence autoencoder for unsupervised representation learning from raw audio files. First, we extract mel-spectrograms from the raw audio files. Second, we train a recurrent sequence to sequence autoencoder on these spectrograms, that are considered as time-dependent frequency vectors. Then, we extract, from a fully connected layer between the decoder and encoder units, the learnt representations of spectrograms as the feature vectors for the corresponding audio instances. Finally, we train a multilayer perceptron neural network on these feature vectors to predict the class labels. In comparison to the baseline, the accuracy is increased from 74:8% to 88:0% on the development set, and from 61:0% to 67:5% on the test set.</p>
<h5>Keywords</h5>
<p class="text-justify">deep feature learning, sequence to sequence learning, recurrent autoencoders, audio processing acoustic scene classification</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexAmiriparian20178ed4f008257e49f2b31719f343bd83ad" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Amiriparian_172.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Amiriparian_172_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o"></i> Slides</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=mcgQWSNaFkI&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexAmiriparian20178ed4f008257e49f2b31719f343bd83adlabel" class="modal fade" id="bibtexAmiriparian20178ed4f008257e49f2b31719f343bd83ad" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexAmiriparian20178ed4f008257e49f2b31719f343bd83adlabel">Sequence to Sequence Autoencoders for Unsupervised Representation Learning from Audio</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Amiriparian2017,
    author = "Amiriparian, Shahin and Freitag, Michael and Cummins, Nicholas and Schuller, Björn",
    title = "Sequence to Sequence Autoencoders for Unsupervised Representation Learning from Audio",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "17--21",
    keywords = "deep feature learning, sequence to sequence learning, recurrent autoencoders, audio processing acoustic scene classification",
    abstract = "This paper describes our contribution to the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2017). We propose a system for this task using a recurrent sequence to sequence autoencoder for unsupervised representation learning from raw audio files. First, we extract mel-spectrograms from the raw audio files. Second, we train a recurrent sequence to sequence autoencoder on these spectrograms, that are considered as time-dependent frequency vectors. Then, we extract, from a fully connected layer between the decoder and encoder units, the learnt representations of spectrograms as the feature vectors for the corresponding audio instances. Finally, we train a multilayer perceptron neural network on these feature vectors to predict the class labels. In comparison to the baseline, the accuracy is increased from 74:8\% to 88:0\% on the development set, and from 61:0\% to 67:5\% on the test set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>15:20</td>
<td class="active">Coffee</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;"><h3 id="coffee-2">Coffee</h3>
<p>Coffee served during the poster session.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>15:20</td>
<td class="success">Posters</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;">
<h3 id="poster-session-i">Poster Session I</h3>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Park2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features</h4><a name="Park2017"></a>
<p>
                    Sangwook Park<sup>1</sup>, Seongkyu Mun<sup>2</sup>, Younglo Lee<sup>1</sup> and Hanseok Ko<sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>School of Electrical Engineering, Korea University, Seoul, Republic of Korea, <sup>2</sup>Department of Visual Information Processing, Korea University, Seoul, Republic of Korea</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Park_214.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapsePark201707822ed213a64f9aae408ce2fbe51668" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsePark201707822ed213a64f9aae408ce2fbe51668" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingPark201707822ed213a64f9aae408ce2fbe51668" class="panel-collapse collapse" id="collapsePark201707822ed213a64f9aae408ce2fbe51668" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This paper proposes new image features for the acoustic scene classification task of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events. In classification of acoustic scenes, identical sounds being observed in different places may affect performance. To resolve this issue, a covariance matrix, which represents energy density for each subband, and a double Fourier transform image, which represents energy variation for each subband, were defined as features. To classify the acoustic scenes with these features, Convolutional Neural Network has been applied with several techniques to reduce training time and to resolve initialization and local optimum problems. According to the experiments which were performed with the DCASE2017 challenge development dataset it is claimed that the proposed method outperformed several baseline methods. Specifically, the class average accuracy is shown as 83.6%, which is an improvement of 8.8%, 9.5%, 8.2% compared to MFCC-MLP, MFCC-GMM, and CepsCom-GMM, respectively.</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustic scene classification, covariance learning, double FFT, convolutional neural network</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexPark201707822ed213a64f9aae408ce2fbe51668" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Park_214.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexPark201707822ed213a64f9aae408ce2fbe51668label" class="modal fade" id="bibtexPark201707822ed213a64f9aae408ce2fbe51668" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexPark201707822ed213a64f9aae408ce2fbe51668label">Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Park2017,
    author = "Park, Sangwook and Mun, Seongkyu and Lee, Younglo and Ko, Hanseok",
    title = "Acoustic Scene Classification Based on Convolutional Neural Network Using Double Image Features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "98--102",
    keywords = "Acoustic scene classification, covariance learning, double FFT, convolutional neural network",
    abstract = "This paper proposes new image features for the acoustic scene classification task of the IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events. In classification of acoustic scenes, identical sounds being observed in different places may affect performance. To resolve this issue, a covariance matrix, which represents energy density for each subband, and a double Fourier transform image, which represents energy variation for each subband, were defined as features. To classify the acoustic scenes with these features, Convolutional Neural Network has been applied with several techniques to reduce training time and to resolve initialization and local optimum problems. According to the experiments which were performed with the DCASE2017 challenge development dataset it is claimed that the proposed method outperformed several baseline methods. Specifically, the class average accuracy is shown as 83.6\%, which is an improvement of 8.8\%, 9.5\%, 8.2\% compared to MFCC-MLP, MFCC-GMM, and CepsCom-GMM, respectively."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Piczak2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification</h4><a name="Piczak2017"></a>
<p>
                    Karol Piczak<br/>
<span class="text-muted"><small><em>Institute of Computer Science, Warsaw University of Technology, Warsaw, Poland</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Piczak_210.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Piczak_210_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapsePiczak2017b2f854134df64b78a8d88aa3fa2cd0ac" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapsePiczak2017b2f854134df64b78a8d88aa3fa2cd0ac" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingPiczak2017b2f854134df64b78a8d88aa3fa2cd0ac" class="panel-collapse collapse" id="collapsePiczak2017b2f854134df64b78a8d88aa3fa2cd0ac" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This study describes a convolutional neural network model submitted to the acoustic scene classification task of the DCASE 2017 challenge. The performance of this model is evaluated with different frequency resolutions of the input spectrogram showing that a higher number of mel bands improves accuracy with negligible impact on the learning time. Additionally, apart from the convolutional model focusing solely on the ambient characteristics of the audio scene, a proposed extension with pretrained event detectors shows potential for further exploration.</p>
<h5>Keywords</h5>
<p class="text-justify">acoustic scene classification, spectrogram, frequency resolution, convolutional neural network, DCASE 2017</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexPiczak2017b2f854134df64b78a8d88aa3fa2cd0ac" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Piczak_210.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Piczak_210_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexPiczak2017b2f854134df64b78a8d88aa3fa2cd0aclabel" class="modal fade" id="bibtexPiczak2017b2f854134df64b78a8d88aa3fa2cd0ac" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexPiczak2017b2f854134df64b78a8d88aa3fa2cd0aclabel">The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Piczak2017,
    author = "Piczak, Karol Jerzy",
    title = "The Details That Matter: Frequency Resolution of Spectrograms in Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "103--107",
    keywords = "acoustic scene classification, spectrogram, frequency resolution, convolutional neural network, DCASE 2017",
    abstract = "This study describes a convolutional neural network model submitted to the acoustic scene classification task of the DCASE 2017 challenge. The performance of this model is evaluated with different frequency resolutions of the input spectrogram showing that a higher number of mel bands improves accuracy with negligible impact on the learning time. Additionally, apart from the convolutional model focusing solely on the ambient characteristics of the audio scene, a proposed extension with pretrained event detectors shows potential for further exploration."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Qian2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Wavelets Revisited for the Classification of Acoustic Scenes</h4><a name="Qian2017"></a>
<p>
                    Qian Kun<sup>1,2,3</sup>, Ren Zhao<sup>2,3</sup>, Pandit Vedhas<sup>2,3</sup>, Yang Zijiang<sup>1,2</sup>, Zhang Zixing<sup>2</sup>, and Schuller Björn<sup>2,3,4</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>MISP group, Technische Universität München, Munich, Germany, <sup>2</sup>Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>3</sup>Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany, <sup>4</sup>GLAM - Group on Language, Audio and Music, Imperial College London, London, UK</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Qian_132.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Qian_132_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseQian2017ea5c3a38a33f4ed7a09a8365a131200a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseQian2017ea5c3a38a33f4ed7a09a8365a131200a" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingQian2017ea5c3a38a33f4ed7a09a8365a131200a" class="panel-collapse collapse" id="collapseQian2017ea5c3a38a33f4ed7a09a8365a131200a" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">We investigate the effectiveness of wavelet features for acoustic scene classification as contribution to the subtask of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017). On the back-end side, gated recurrent neural networks (GRNNs) are compared against traditional support vector machines (SVMs). We observe that, the proposed wavelet features behave comparable to the typically-used temporal and spectral features in the classification of acoustic scenes. Further, a late fusion of trained models with wavelets and typical acoustic features reach the best averaged 4-fold cross validation accuracy of 83.2 \%, and 82.6 \% by SVMs, and GRNNs, respectively; both significantly outperform the baseline (74.8 \%) of the official development set (p &lt; 0:001, one-tailed z-test).</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustic Scene Classification, Wavelets, Support Vector Machines, Sequence Modelling, Gated Recurrent Neural Networks</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexQian2017ea5c3a38a33f4ed7a09a8365a131200a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Qian_132.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Qian_132_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexQian2017ea5c3a38a33f4ed7a09a8365a131200alabel" class="modal fade" id="bibtexQian2017ea5c3a38a33f4ed7a09a8365a131200a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexQian2017ea5c3a38a33f4ed7a09a8365a131200alabel">Wavelets Revisited for the Classification of Acoustic Scenes</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Qian2017,
    author = "Qian, Kun and Ren, Zhao and Pandit, Vedhas and Yang, Zijiang and Zhang, Zixing and Schuller, Björn",
    title = "Wavelets Revisited for the Classification of Acoustic Scenes",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "108--112",
    keywords = "Acoustic Scene Classification, Wavelets, Support Vector Machines, Sequence Modelling, Gated Recurrent Neural Networks",
    abstract = "We investigate the effectiveness of wavelet features for acoustic scene classification as contribution to the subtask of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017). On the back-end side, gated recurrent neural networks (GRNNs) are compared against traditional support vector machines (SVMs). We observe that, the proposed wavelet features behave comparable to the typically-used temporal and spectral features in the classification of acoustic scenes. Further, a late fusion of trained models with wavelets and typical acoustic features reach the best averaged 4-fold cross validation accuracy of 83.2 \\%, and 82.6 \\% by SVMs, and GRNNs, respectively; both significantly outperform the baseline (74.8 \\%) of the official development set (p &lt; 0:001, one-tailed z-test)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Ren2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Deep Sequential Image Features on Acoustic Scene Classification</h4><a name="Ren2017"></a>
<p>
                    Ren Zhao<sup>1,2</sup>, Pandit Vedhas<sup>1,2</sup>, Qian Kun<sup>1,2,3</sup>, Yang Zijiang<sup>1,2</sup>, Zhang Zixing<sup>2</sup>, and Schuller Björn<sup>1,2,4</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany, <sup>2</sup>Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany, <sup>3</sup>MISP group, Technische Universität München, Munich, Germany, <sup>4</sup>GLAM - Group on Language, Audio and Music, Imperial College London, London, UK</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Ren_133.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Ren_133_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseRen20174d5fc84f8a4f4a2aba1f0336cc577f27" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseRen20174d5fc84f8a4f4a2aba1f0336cc577f27" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingRen20174d5fc84f8a4f4a2aba1f0336cc577f27" class="panel-collapse collapse" id="collapseRen20174d5fc84f8a4f4a2aba1f0336cc577f27" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">For the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017), we propose a novel method to classify 15 different acoustic scenes using deep sequential learning, based on features extracted from Short-Time Fourier Transform and scalogram of the audio scenes using Convolutional Neural Networks. It is the first time to investigate the performance of bump and morse scalograms for acoustic scene classification in an according context. First, segmented audio waves are transformed into a spectrogram and two types of scalograms; then, ‘deep features’ are extracted from these using the pre-trained VGG16 model by probing at the fully connected layer. These representations are then fed into Gated Recurrent Neural Networks for classification separately. Predictions from the three systems are finally combined by a margin sampling value strategy. On the official development set of the challenge, the best accuracy on a four-fold cross-validation setup is 80:9%, which increases by 6:1% when compared with the official baseline (p &lt; :001 by one-tailed z-test).</p>
<h5>Keywords</h5>
<p class="text-justify">Audio Scene Classification, Deep Sequential Learning, Scalogram, Convolutional Neural Networks, Gated Recurrent Neural Networks</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexRen20174d5fc84f8a4f4a2aba1f0336cc577f27" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Ren_133.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Ren_133_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexRen20174d5fc84f8a4f4a2aba1f0336cc577f27label" class="modal fade" id="bibtexRen20174d5fc84f8a4f4a2aba1f0336cc577f27" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexRen20174d5fc84f8a4f4a2aba1f0336cc577f27label">Deep Sequential Image Features on Acoustic Scene Classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Ren2017,
    author = "Ren, Zhao and Pandit, Vedhas and Qian, Kun and Yang, Zijiang and Zhang, Zixing and Schuller, Björn",
    title = "Deep Sequential Image Features on Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "113--117",
    keywords = "Audio Scene Classification, Deep Sequential Learning, Scalogram, Convolutional Neural Networks, Gated Recurrent Neural Networks",
    abstract = "For the Acoustic Scene Classification task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017), we propose a novel method to classify 15 different acoustic scenes using deep sequential learning, based on features extracted from Short-Time Fourier Transform and scalogram of the audio scenes using Convolutional Neural Networks. It is the first time to investigate the performance of bump and morse scalograms for acoustic scene classification in an according context. First, segmented audio waves are transformed into a spectrogram and two types of scalograms; then, ‘deep features’ are extracted from these using the pre-trained VGG16 model by probing at the fully connected layer. These representations are then fed into Gated Recurrent Neural Networks for classification separately. Predictions from the three systems are finally combined by a margin sampling value strategy. On the official development set of the challenge, the best accuracy on a four-fold cross-validation setup is 80:9\%, which increases by 6:1\% when compared with the official baseline (p &lt; :001 by one-tailed z-test)."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Schindler2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Multi-Temporal Resolution Convolutional Neural Networks for Acoustic Scene Classification</h4><a name="Schindler2017"></a>
<p>
                    Alexander Schindler<sup>1</sup>, Thomas Lidy<sup>2</sup> and Andreas Rauber<sup>2</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Center for Digital Safety and Security, Austrian Institute of Technology, Vienna, Austria, <sup>2</sup>Institute for Software and Interactive Systems, Technical University of Vienna, Vienna, Austria</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Schindler_152.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Schindler_152_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseSchindler20170c4399d8ae9c4430a3c69d6aefc5722d" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseSchindler20170c4399d8ae9c4430a3c69d6aefc5722d" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingSchindler20170c4399d8ae9c4430a3c69d6aefc5722d" class="panel-collapse collapse" id="collapseSchindler20170c4399d8ae9c4430a3c69d6aefc5722d" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">In this paper we present a Deep Neural Network architecture for the task of acoustic scene classification which harnesses information from increasing temporal resolutions of Mel-Spectrogram segments. This architecture is composed of separated parallel Convolutional Neural Networks which learn spectral and temporal representations for each input resolution. The resolutions are chosen to cover fine-grained characteristics of a scene’s spectral texture as well as its distribution of acoustic events. The proposed model shows a 3.56% absolute improvement of the best performing single resolution model and 12.49% of the DCASE 2017 Acoustic Scenes Classification task baseline [1].</p>
<h5>Keywords</h5>
<p class="text-justify">Deep Learning, Convolutional Neural Networks, Acoustic Scene Classification, Audio Analysis</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexSchindler20170c4399d8ae9c4430a3c69d6aefc5722d" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Schindler_152.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Schindler_152_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexSchindler20170c4399d8ae9c4430a3c69d6aefc5722dlabel" class="modal fade" id="bibtexSchindler20170c4399d8ae9c4430a3c69d6aefc5722d" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexSchindler20170c4399d8ae9c4430a3c69d6aefc5722dlabel">Multi-Temporal Resolution Convolutional Neural Networks for Acoustic Scene Classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Schindler2017,
    author = "Schindler, Alexander and Lidy, Thomas and Rauber, Andreas",
    title = "Multi-Temporal Resolution Convolutional Neural Networks for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "118--122",
    keywords = "Deep Learning, Convolutional Neural Networks, Acoustic Scene Classification, Audio Analysis",
    abstract = "In this paper we present a Deep Neural Network architecture for the task of acoustic scene classification which harnesses information from increasing temporal resolutions of Mel-Spectrogram segments. This architecture is composed of separated parallel Convolutional Neural Networks which learn spectral and temporal representations for each input resolution. The resolutions are chosen to cover fine-grained characteristics of a scene’s spectral texture as well as its distribution of acoustic events. The proposed model shows a 3.56\% absolute improvement of the best performing single resolution model and 12.49\% of the DCASE 2017 Acoustic Scenes Classification task baseline [1]."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Cakir2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Convolutional Recurrent Neural Networks for Rare Sound Event Detection</h4><a name="Cakir2017"></a>
<p>
                    Emre Cakir and Tuomas Virtanen<br/>
<span class="text-muted"><small><em>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Cakir_105.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapseCakir2017b2f0b02ccb654f79abcc1ac7f637cd32" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseCakir2017b2f0b02ccb654f79abcc1ac7f637cd32" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingCakir2017b2f0b02ccb654f79abcc1ac7f637cd32" class="panel-collapse collapse" id="collapseCakir2017b2f0b02ccb654f79abcc1ac7f637cd32" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Sound events possess certain temporal and spectral structure in their time-frequency representations. The spectral content for the samples of the same sound event class may exhibit small shifts due to intra-class acoustic variability. Convolutional layers can be used to learn high-level, shift invariant features from time-frequency representations of acoustic samples, while recurrent layers can be used to learn the longer term temporal context from the extracted high-level features. In this paper, we propose combining these two in a convolutional recurrent neural network (CRNN) for rare sound event detection. The proposed method is evaluated over DCASE 2017 challenge dataset of individual sound event samples mixed with everyday acoustic scene samples. CRNN provides significant performance improvement over two other deep learning based methods mainly due to its capability of longer term temporal modeling.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound Event Detection, Convolutional Neural Network, Recurrent Neural Network, Machine learning</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexCakir2017b2f0b02ccb654f79abcc1ac7f637cd32" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Cakir_105.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexCakir2017b2f0b02ccb654f79abcc1ac7f637cd32label" class="modal fade" id="bibtexCakir2017b2f0b02ccb654f79abcc1ac7f637cd32" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexCakir2017b2f0b02ccb654f79abcc1ac7f637cd32label">Convolutional Recurrent Neural Networks for Rare Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Cakir2017,
    author = "Cakir, Emre and Virtanen, Tuomas",
    title = "Convolutional Recurrent Neural Networks for Rare Sound Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "27--31",
    keywords = "Sound Event Detection, Convolutional Neural Network, Recurrent Neural Network, Machine learning",
    abstract = "Sound events possess certain temporal and spectral structure in their time-frequency representations. The spectral content for the samples of the same sound event class may exhibit small shifts due to intra-class acoustic variability. Convolutional layers can be used to learn high-level, shift invariant features from time-frequency representations of acoustic samples, while recurrent layers can be used to learn the longer term temporal context from the extracted high-level features. In this paper, we propose combining these two in a convolutional recurrent neural network (CRNN) for rare sound event detection. The proposed method is evaluated over DCASE 2017 challenge dataset of individual sound event samples mixed with everyday acoustic scene samples. CRNN provides significant performance improvement over two other deep learning based methods mainly due to its capability of longer term temporal modeling."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Lim2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Rare Sound Event Detection Using 1D Convolutional Recurrent Neural Networks</h4><a name="Lim2017"></a>
<p>
                    Hyungui Lim<sup>1</sup>, Jeongsoo Park<sup>2,3</sup> and Yoonchang Han<sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Cochlear.ai, Seoul, Korea, <sup>2</sup>N/A, Cochlear.ai, Seoul, Korea, <sup>3</sup>Music and Audio Research Group, Seoul National University, Seoul, Korea</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lim_205.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Lim_205_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseLim20174209217bc831447181a8c9005e3edfcb" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseLim20174209217bc831447181a8c9005e3edfcb" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingLim20174209217bc831447181a8c9005e3edfcb" class="panel-collapse collapse" id="collapseLim20174209217bc831447181a8c9005e3edfcb" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Rare sound event detection is a newly proposed task in IEEE DCASE 2017 to identify the presence of monophonic sound event that is classified as an emergency and to detect the onset time of the event. In this paper, we introduce a rare sound event detection system using combination of 1D convolutional neural network (1D ConvNet) and recurrent neural network (RNN) with long shortterm memory units (LSTM). A log-amplitude mel-spectrogram is used as an input acoustic feature and the 1D ConvNet is applied in each time-frequency frame to convert the spectral feature. Then the RNN-LSTM is utilized to incorporate the temporal dependency of the extracted features. The system is evaluated using DCASE 2017 Challenge Task 2 Dataset. Our best result on the test set of the development dataset shows 0.07 and 96.26 of error rate and F-score on the event-based metric, respectively. The proposed system has achieved the 1st place in the challenge with an error rate of 0.13 and an F-Score of 93.1 on the evaluation dataset.</p>
<h5>Keywords</h5>
<p class="text-justify">Rare sound event detection, deep learning, convolutional neural network, recurrent neural network, long short-term memory</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexLim20174209217bc831447181a8c9005e3edfcb" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lim_205.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Lim_205_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexLim20174209217bc831447181a8c9005e3edfcblabel" class="modal fade" id="bibtexLim20174209217bc831447181a8c9005e3edfcb" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexLim20174209217bc831447181a8c9005e3edfcblabel">Rare Sound Event Detection Using 1D Convolutional Recurrent Neural Networks</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lim2017,
    author = "Lim, Hyungui and Park, Jeongsoo and Han, Yoonchang",
    title = "Rare Sound Event Detection Using {1D} Convolutional Recurrent Neural Networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "80--84",
    keywords = "Rare sound event detection, deep learning, convolutional neural network, recurrent neural network, long short-term memory",
    abstract = "Rare sound event detection is a newly proposed task in IEEE DCASE 2017 to identify the presence of monophonic sound event that is classified as an emergency and to detect the onset time of the event. In this paper, we introduce a rare sound event detection system using combination of 1D convolutional neural network (1D ConvNet) and recurrent neural network (RNN) with long shortterm memory units (LSTM). A log-amplitude mel-spectrogram is used as an input acoustic feature and the 1D ConvNet is applied in each time-frequency frame to convert the spectral feature. Then the RNN-LSTM is utilized to incorporate the temporal dependency of the extracted features. The system is evaluated using DCASE 2017 Challenge Task 2 Dataset. Our best result on the test set of the development dataset shows 0.07 and 96.26 of error rate and F-score on the event-based metric, respectively. The proposed system has achieved the 1st place in the challenge with an error rate of 0.13 and an F-Score of 93.1 on the evaluation dataset."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;"><h3 id="dcase2017-challenge-posters"><strong>DCASE2017 Challenge posters</strong></h3></td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="DCASE2017_RESULTS" data-source="content/data/workshop2017/extra.bib" data-template="fancy_minimal_no_bibtex">
<div class="row">
<div class="col-md-9">
<h4>DCASE2017 Challenge Results</h4><a name="DCASE2017_RESULTS"></a>
<p>
                    Annamaria Mesaros<sup>1</sup>, Toni Heittola<sup>1</sup>, Aleksandr Diment<sup>1</sup>, Benjamin Elizalde<sup>2</sup>, Ankit Shah<sup>2</sup>, Emmanuel Vincent<sup>3</sup>, Bhiksha Raj<sup>2</sup> and Tuomas Virtanen <sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Tampere University of Technology, Laboratory of Signal Processing, Tampere, Finland, <sup>2</sup>Carnegie Mellon University, Department of Electrical and Computer Engineering, &amp; Department of Language Technologies Institute, Pittsburgh, USA, <sup>3</sup>Inria, F-54600 Villers-les-Nancy, France</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<button aria-controls="collapseDCASE2017_RESULTS68e9e4fcae6a416fb9251eedb8b9e7c2" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDCASE2017_RESULTS68e9e4fcae6a416fb9251eedb8b9e7c2" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDCASE2017_RESULTS68e9e4fcae6a416fb9251eedb8b9e7c2" class="panel-collapse collapse" id="collapseDCASE2017_RESULTS68e9e4fcae6a416fb9251eedb8b9e7c2" role="tabpanel">
<h5>Keywords</h5>
<p class="text-justify">Sound scene analysis, Acoustic scene classification, Sound event detection, Audio tagging, Rare sound events, Weak Labels</p>
<div class="btn-group">
</div>
<div class="btn-group">
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Lehner2017" data-source="content/data/challenge2017/technical_reports_task1.bib" data-template="fancy_minimal_no_bibtex">
<div class="row">
<div class="col-md-9">
<h4>Classifying Short Acoustic Scenes with I-Vectors and CNNs: Challenges and Optimisations for the 2017 DCASE ASC Task</h4><a name="Lehner2017"></a>
<p>
                    Bernhard Lehner, Hamid Eghbal-Zadeh, Matthias Dorfer, Filip Korzeniowski, Khaled Koutini and Gerhard Widmer<br/>
<span class="text-muted"><small><em>Department of Computational Perception, Johannes Kepler University, Linz, Austria</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Lehner_142.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapseLehner201709f83d3479e446a7a3effc69322cff82" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseLehner201709f83d3479e446a7a3effc69322cff82" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingLehner201709f83d3479e446a7a3effc69322cff82" class="panel-collapse collapse" id="collapseLehner201709f83d3479e446a7a3effc69322cff82" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This report describes the CP-JKU team's submissions for Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2017 challenge, and discusses some observations we made about the data and the classification setup. Our approach is based on the methodology that achieved ranks 1 and 2 in the 2016 ASC challenge: a fusion of i-vector modelling using MFCC features derived from left and right audio channels, and deep convolutional neural networks (CNNs) trained on raw spectrograms. The data provided for the 2017 ASC task presented some new challenges -- in particular, audio stimuli of very short duration. These will be discussed in detail, and our measures for addressing them will be described. The result of our experiments is a classification system that achieves classification accuracies of around 90% on the provided development data, as estimated via the prescribed four-fold cross-validation scheme (which, we suspect, may be rather optimistic in relation to new data).</p>
<div class="btn-group">
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Lehner_142.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Adavanne2017" data-source="content/data/challenge2017/technical_reports_task3.bib" data-template="fancy_minimal_no_bibtex">
<div class="row">
<div class="col-md-9">
<h4>A Report on Sound Event Detection with Different Binaural Features</h4><a name="Adavanne2017"></a>
<p>
                    Sharath Adavanne and Tuomas Virtanen<br/>
<span class="text-muted"><small><em>Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Adavanne_130.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapseAdavanne2017051c40e0518b4980904b88c99b462140" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseAdavanne2017051c40e0518b4980904b88c99b462140" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingAdavanne2017051c40e0518b4980904b88c99b462140" class="panel-collapse collapse" id="collapseAdavanne2017051c40e0518b4980904b88c99b462140" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">In this paper, we compare the performance of using binaural audio features in place of single channel features for sound event detection. Three different binaural features are studied and evaluated on the publicly available TUT Sound Events 2017 dataset of length 70 minutes. Sound event detection is performed separately with single channel and binaural features using stacked convolutional and recurrent neural network and the evaluation is reported using standard metrics of error rate and F-score. The studied binaural features are seen to consistently perform equal to or better than the single-channel features with respect to error rate metric.</p>
<div class="btn-group">
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Adavanne_130.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Xu2017" data-source="content/data/challenge2017/technical_reports_task4.bib" data-template="fancy_minimal_no_bibtex">
<div class="row">
<div class="col-md-9">
<h4>Surrey-CVSSP System for DCASE2017 Challenge Task4</h4><a name="Xu2017"></a>
<p>
                    Yong Xu, Qiuqiang Kong, Wenwu Wang and Mark D. Plumbley<br/>
<span class="text-muted"><small><em>Center for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Xu_146.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapseXu2017309db738ea234a11aab26fc6079903ec" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseXu2017309db738ea234a11aab26fc6079903ec" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingXu2017309db738ea234a11aab26fc6079903ec" class="panel-collapse collapse" id="collapseXu2017309db738ea234a11aab26fc6079903ec" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">In this technique report, we present a bunch of methods for the task 4 of Detection and Classification of Acoustic Scenes and Events 2017 (DCASE2017) challenge. This task evaluates systems for the large-scale detection of sound events using weakly labeled training data. The data are YouTube video excerpts focusing on transportation and warnings due to their industry applications. There are two tasks, audio tagging and sound event detection from weakly labeled data. Convolutional neural network (CNN) and gated recurrent unit (GRU) based recurrent neural network (RNN) are adopted as our basic framework. We proposed a learnable gating activation function for selecting informative local features. Attention-based scheme is used for localizing the specific events in a weakly-supervised mode. A new batch-level balancing strategy is also proposed to tackle the data unbalancing problem. Fusion of posteriors from different systems are found effective to improve the performance. In a summary, we get 61% F-value for the audio tagging subtask and 0.72 error rate (ER) for the sound event detection subtask on the development set. While the official multilayer perceptron (MLP) based baseline just obtained 13.1% F-value for the audio tagging and 1.02 for the sound event detection.</p>
<div class="btn-group">
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Xu_146.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" href="https://github.com/yongxuUSTC/dcase2017_task4_cvssp" title="Source code"><i class="fa fa-file-code-o"></i> Source code</a>
</div>
</div>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>17:00</td>
<td class="danger">Discussion</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;">
<h3 id="open-discussion">Open Discussion</h3>
<p>Moderated by Mark Plumbley <br/><span class="text-muted"><em>University of Surrey, United Kingdom</em></span></p>
</td>
</tr>
</table>
</td>
</tr>
</tbodv></table>
<h2 id="day-2">Day 2</h2>
<p><small><i aria-hidden="true" class="fa fa-clock-o"></i> Friday 17.11.2017, 9:00 - 12:10</small></p>
<table class="table table-condensed schedule">
<thead>
<tr>
<th style="width:50px;">Hours</th>
<th style="width:100px;"></th>
<th></th>
</tr>
</thead>
<tbodv>
<tr>
<td>9:00</td>
<td class="danger">Keynote</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;">
<h3 id="keynote-1">Keynote</h3>
<p class="text-muted"><em>Session chair Dan Stowell<em></em></em></p>
<div class="btex-item" data-item="McDermott_KEYNOTE" data-source="content/data/workshop2017/extra.bib" data-template="fancy_minimal_keynote">
<div class="row">
<div class="col-md-9">
<h4>Sound Texture Perception via Summary Statistics</h4><a name="McDermott_KEYNOTE"></a>
<p>
                    Josh McDermott<br/>
<span class="text-muted"><small><em>Massachusetts Institute of Technology, USA</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=P3V5Q-9WRZw&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseMcDermott_KEYNOTE769f6a41a11a413394a17a66d4426226" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseMcDermott_KEYNOTE769f6a41a11a413394a17a66d4426226" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingMcDermott_KEYNOTE769f6a41a11a413394a17a66d4426226" class="panel-collapse collapse" id="collapseMcDermott_KEYNOTE769f6a41a11a413394a17a66d4426226" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Sound textures are produced by superpositions of large numbers of similar acoustic features (as in rain, swarms of insects, or galloping horses). Textures are noteworthy for being stationary, raising the possibility that time-averaged statistics might capture their structure. I will describe several lines of work testing this idea. I will show how the synthesis of textures from statistics of biological auditory models provides evidence for statistical texture representations. I will then describe experiments that characterize the process by which texture statistics are measured by the auditory system, and that explore their role in auditory scene analysis.</p>
<h5>Biography</h5>
<p class="text-justify">Josh McDermott is a perceptual scientist studying sound and hearing in the Department of Brain and Cognitive Sciences at MIT, where he is the Fred &amp; Carole Middleton Career Development Assistant Professor and heads the Laboratory for Computational Audition. His research addresses human and machine audition using tools from experimental psychology, engineering, and neuroscience. McDermott obtained a BA in Brain and Cognitive Science from Harvard, an MPhil in Computational Neuroscience from University College London, a PhD in Brain and Cognitive Science from MIT, and postdoctoral training in psychoacoustics at the University of Minnesota and in computational neuroscience at NYU. He is the recipient of a Marshall Scholarship, a James S. McDonnell Foundation Scholar Award, and an NSF CAREER Award.</p>
<div class="row">
<div class="col-md-10">
<h5><strong>Josh McDermott</strong></h5>
<p><em>
                    
                        Fred &amp; Carole Middleton Career Development Assistant Professor, Department of Brain and Cognitive Science, Massachusetts Institute of Technology, USA
                              
                    </em></p>
</div>
<div class="col-md-2">
<img class="img img-rounded" src="https://dcase.community/images/person/josh_mcdermott.jpg"/>
</div>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=P3V5Q-9WRZw&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>9:50</td>
<td class="success">Presentations</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;">
<h3 id="oral-session-iii">Oral Session III</h3>
<p class="text-muted"><em>Session chair Dan Stowell<em></em></em></p>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">9:50</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Dekkers2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network</h4><a name="Dekkers2017"></a>
<p>
                    Gert Dekkers<sup>1,2</sup>, Steven Lauwereins<sup>2</sup>, Bart Thoen<sup>1</sup>, Mulu Weldegebreal Adhana<sup>1</sup>, Henk Brouckxon<sup>3</sup>, Bertold Van den Bergh<sup>2</sup>, Toon van Waterschoot<sup>1,2</sup>, Bart Vanrumste<sup>1,2,4</sup>, Marian Verhelst<sup>2</sup>, Peter Karsmakers<sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup> KU Leuven, Department of Electrical Engineering, Engineering Technology Cluster, Geel, Belgium, <sup>2</sup> KU Leuven, Department of Electrical Engineering, Leuven, Belgium, <sup>3</sup> Vrije Universiteit Brussel, Department ETRO-DSSP, Brussels, Belgium, <sup>4</sup> IMEC, Leuven, Belgium</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=ggInY7B7Enw&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR]" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="https://iiw.kuleuven.be/onderzoek/advise/datasets#SINS%20database" rel="tooltip" title=""><i class="fa fa-database"></i></a>
<button aria-controls="collapseDekkers2017d592cc8760c34fd2a2c3b1ca9be0cc22" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseDekkers2017d592cc8760c34fd2a2c3b1ca9be0cc22" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingDekkers2017d592cc8760c34fd2a2c3b1ca9be0cc22" class="panel-collapse collapse" id="collapseDekkers2017d592cc8760c34fd2a2c3b1ca9be0cc22" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research.</p>
<h5>Keywords</h5>
<p class="text-justify">Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexDekkers2017d592cc8760c34fd2a2c3b1ca9be0cc22" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Dekkers_141.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=ggInY7B7Enw&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR]" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="https://iiw.kuleuven.be/onderzoek/advise/datasets#SINS%20database" rel="tooltip" title="Toolbox"><i class="fa fa-database"></i> </a>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexDekkers2017d592cc8760c34fd2a2c3b1ca9be0cc22label" class="modal fade" id="bibtexDekkers2017d592cc8760c34fd2a2c3b1ca9be0cc22" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexDekkers2017d592cc8760c34fd2a2c3b1ca9be0cc22label">The SINS Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Dekkers2017,
    author = "Dekkers, Gert and Lauwereins, Steven and Thoen, Bart and Adhana, Mulu Weldegebreal and Brouckxon, Henk and van Waterschoot, Toon and Vanrumste, Bart and Verhelst, Marian and Karsmakers, Peter and",
    title = "The {SINS} Database for Detection of Daily Activities in a Home Environment Using an Acoustic Sensor Network",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "32--36",
    keywords = "Database, Acoustic Scene Classification, Acoustic Event Detection, Acoustic Sensor Networks",
    abstract = "There is a rising interest in monitoring and improving human wellbeing at home using different types of sensors including microphones. In the context of Ambient Assisted Living (AAL) persons are monitored, e.g. to support patients with a chronic illness and older persons, by tracking their activities being performed at home. When considering an acoustic sensing modality, a performed activity can be seen as an acoustic scene. Recently, acoustic detection and classification of scenes and events has gained interest in the scientific community and led to numerous public databases for a wide range of applications. However, no public databases exist which a) focus on daily activities in a home environment, b) contain activities being performed in a spontaneous manner, c) make use of an acoustic sensor network, and d) are recorded as a continuous stream. In this paper we introduce a database recorded in one living home, over a period of one week. The recording setup is an acoustic sensor network containing thirteen sensor nodes, with four low-cost microphones each, distributed over five rooms. Annotation is available on an activity level. In this paper we present the recording and annotation procedure, the database content and a discussion on a baseline detection benchmark. The baseline consists of Mel-Frequency Cepstral Coefficients, Support Vector Machine and a majority vote late-fusion scheme. The database is publicly released to provide a common ground for future research."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">10:10</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Green2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Acoustic Scene Classification Using Spatial Features</h4><a name="Green2017"></a>
<p>
                    Marc C. Green and Damian Murphy<br/>
<span class="text-muted"><small><em>Audio Lab, Department of Electonic Engineering, University of York, York, UK</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Green_126.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Green_126_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o fa-1x"></i></a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=7h-isSRk6o0&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="https://zenodo.org/record/1012809#.WhU8KRZLeoo" rel="tooltip" title="EigenScape"><i class="fa fa-database"></i></a>
<button aria-controls="collapseGreen2017a409717561114ff5a7c3feb7877891f9" aria-expanded="true" class="btn btn-xs btn-success" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseGreen2017a409717561114ff5a7c3feb7877891f9" type="button">
<i class="fa fa-git"></i>
</button>
<button aria-controls="collapseGreen2017a409717561114ff5a7c3feb7877891f9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseGreen2017a409717561114ff5a7c3feb7877891f9" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingGreen2017a409717561114ff5a7c3feb7877891f9" class="panel-collapse collapse" id="collapseGreen2017a409717561114ff5a7c3feb7877891f9" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Due to various factors, the vast majority of the research in the field of Acoustic Scene Classification has used monaural or binaural datasets. This paper introduces EigenScape - a new dataset of 4th-order Ambisonic acoustic scene recordings - and presents preliminary analysis of this dataset. The data is classified using a standard Mel-Frequency Cepstral Coefficient - Gaussian Mixture Model system, and the performance of this system is compared to that of a new system using spatial features extracted using Directional Audio Coding (DirAC) techniques. The DirAC features are shown to perform well in scene classification, with some subsets of these features outperforming the MFCC classification. The differences in label confusion between the two systems are especially interesting, as these suggest that certain scenes that are spectrally similar might not necessarily be spatially similar.</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustic scene classification, MFCC, gaussian mixture model, ambisonics, directional audio coding, multichannel, eigenmike</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexGreen2017a409717561114ff5a7c3feb7877891f9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Green_126.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Green_126_presentation.pdf" rel="tooltip" title="Download slides"><i class="fa fa-picture-o"></i> Slides</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=7h-isSRk6o0&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="https://zenodo.org/record/1012809#.WhU8KRZLeoo" rel="tooltip" title="Toolbox"><i class="fa fa-database"></i> EigenScape</a>
<a class="btn btn-sm btn-success" href="https://github.com/marc1701/EigenScape" style="text-decoration:none;border-bottom:0;padding-bottom:9px" title="EigenScape"><i class="fa fa-git"></i> EigenScape</a>
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexGreen2017a409717561114ff5a7c3feb7877891f9label" class="modal fade" id="bibtexGreen2017a409717561114ff5a7c3feb7877891f9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexGreen2017a409717561114ff5a7c3feb7877891f9label">Acoustic Scene Classification Using Spatial Features</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Green2017,
    author = "Green, Marc Ciufo and Murphy, Damian",
    title = "Acoustic Scene Classification Using Spatial Features",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "42--45",
    keywords = "Acoustic scene classification, MFCC, gaussian mixture model, ambisonics, directional audio coding, multichannel, eigenmike",
    abstract = "Due to various factors, the vast majority of the research in the field of Acoustic Scene Classification has used monaural or binaural datasets. This paper introduces EigenScape - a new dataset of 4th-order Ambisonic acoustic scene recordings - and presents preliminary analysis of this dataset. The data is classified using a standard Mel-Frequency Cepstral Coefficient - Gaussian Mixture Model system, and the performance of this system is compared to that of a new system using spatial features extracted using Directional Audio Coding (DirAC) techniques. The DirAC features are shown to perform well in scene classification, with some subsets of these features outperforming the MFCC classification. The differences in label confusion between the two systems are especially interesting, as these suggest that certain scenes that are spectrally similar might not necessarily be spatially similar."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;">10:30</td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Abesser2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Acoustic Scene Classification by Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks</h4><a name="Abesser2017"></a>
<p>
                    Jakob Abeßer, Stylianos Ioannis Mimilakis, Robert Grafe, and Hanna Lukashevich<br/>
<span class="text-muted"><small><em>Fraunhofer IDMT, Ilmenau, Germany</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Abesser_165.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-success btn-btex" data-placement="bottom" href="https://www.youtube.com/watch?v=25VCPLKgM4Y&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera fa-1x"></i></a>
<button aria-controls="collapseAbesser20173bcf1babe1934f53980dba87d56cbc59" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseAbesser20173bcf1babe1934f53980dba87d56cbc59" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingAbesser20173bcf1babe1934f53980dba87d56cbc59" class="panel-collapse collapse" id="collapseAbesser20173bcf1babe1934f53980dba87d56cbc59" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Motivated by the recent success of deep learning techniques in various audio analysis tasks, this work presents a distributed sensor-server system for acoustic scene classification in urban environments based on deep convolutional neural networks (CNN). Stacked autoencoders are used to compress extracted spectrogram patches on the sensor side before being transmitted to and classified on the server side. In our experiments, we compare two state-of-theart CNN architectures subject to their classification accuracy under the presence of environmental noise, the dimensionality reduction in the encoding stage, as well as a reduced number of filters in the convolution layers. Our results show that the best model configuration leads to a classification accuracy of 75% for 5 acoustic scenes. We furthermore discuss which confusions among particular classes can be ascribed to particular sound event types, which are present in multiple acoustic scene classes.</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustic Scene Classification, Convolutional Neural Networks, Stacked Denoising Autoencoder, Smart City</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexAbesser20173bcf1babe1934f53980dba87d56cbc59" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Abesser_165.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-success btn-btex2" data-placement="bottom" href="https://www.youtube.com/watch?v=25VCPLKgM4Y&amp;list=PLZ4kfMr-5Z2xrMKdEQii_2c-5HyKcPKcR" rel="tooltip" title="Video"><i class="fa fa-video-camera"></i> Video</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexAbesser20173bcf1babe1934f53980dba87d56cbc59label" class="modal fade" id="bibtexAbesser20173bcf1babe1934f53980dba87d56cbc59" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexAbesser20173bcf1babe1934f53980dba87d56cbc59label">Acoustic Scene Classification by Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Abesser2017,
    author = "Abeßer, Jakob and Mimilakis, Stylianos Ioannis and Gräfe, Robert and Lukashevich, Hanna",
    title = "Acoustic Scene Classification by Combining Autoencoder-Based Dimensionality Reduction and Convolutional Neural Networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "7--11",
    keywords = "Acoustic Scene Classification, Convolutional Neural Networks, Stacked Denoising Autoencoder, Smart City",
    abstract = "Motivated by the recent success of deep learning techniques in various audio analysis tasks, this work presents a distributed sensor-server system for acoustic scene classification in urban environments based on deep convolutional neural networks (CNN). Stacked autoencoders are used to compress extracted spectrogram patches on the sensor side before being transmitted to and classified on the server side. In our experiments, we compare two state-of-theart CNN architectures subject to their classification accuracy under the presence of environmental noise, the dimensionality reduction in the encoding stage, as well as a reduced number of filters in the convolution layers. Our results show that the best model configuration leads to a classification accuracy of 75\% for 5 acoustic scenes. We furthermore discuss which confusions among particular classes can be ascribed to particular sound event types, which are present in multiple acoustic scene classes."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>10:50</td>
<td class="active">Coffee</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;"><h3 id="coffee-3">Coffee</h3>
<p>Coffee served during the poster session.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>10:50</td>
<td class="success">Posters</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;">
<h3 id="poster-session-ii">Poster Session II</h3>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Han2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification</h4><a name="Han2017"></a>
<p>
                    Yoonchang Han<sup>1</sup> and Jeongsoo Park<sup>1,2</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Cochlear.ai, Seoul, Korea, <sup>2</sup>Music and Audio Research Group, Seoul National University, Seoul, Korea</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Han_206.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Han_206_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseHan201788d96fc0de364f9582cd931bb1e48c4c" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseHan201788d96fc0de364f9582cd931bb1e48c4c" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingHan201788d96fc0de364f9582cd931bb1e48c4c" class="panel-collapse collapse" id="collapseHan201788d96fc0de364f9582cd931bb1e48c4c" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">In this paper, we demonstrate how we applied convolutional neural network for DCASE 2017 task 1, acoustic scene classification. We propose a variety of preprocessing methods that emphasise different acoustic characteristics such as binaural representations, harmonicpercussive source separation, and background subtraction. We also present a network structure designed for paired input to make the most of the spatial information contained in the stereo. The experimental results show that the proposed network structures and the preprocessing methods effectively learn acoustic characteristics from the audio recordings, and their ensemble model significantly reduces the error rate further, exhibiting an accuracy of 0.917 for 4-fold cross-validation on the development. The proposed system achieved second place in DCASE 2017 task 1 with an accuracy of 0.804 on the evaluation set.</p>
<h5>Keywords</h5>
<p class="text-justify">DCASE 2017, acoustic scene classification, convolutional neural network, binaural representations, harmonicpercussive source separation, background subtraction</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexHan201788d96fc0de364f9582cd931bb1e48c4c" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Han_206.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Han_206_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexHan201788d96fc0de364f9582cd931bb1e48c4clabel" class="modal fade" id="bibtexHan201788d96fc0de364f9582cd931bb1e48c4c" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexHan201788d96fc0de364f9582cd931bb1e48c4clabel">Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Han2017,
    author = "Han, Yoonchang and Park, Jeongsoo",
    title = "Convolutional Neural Networks with Binaural Representations and Background Subtraction for Acoustic Scene Classification",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "46--50",
    keywords = "DCASE 2017, acoustic scene classification, convolutional neural network, binaural representations, harmonicpercussive source separation, background subtraction",
    abstract = "In this paper, we demonstrate how we applied convolutional neural network for DCASE 2017 task 1, acoustic scene classification. We propose a variety of preprocessing methods that emphasise different acoustic characteristics such as binaural representations, harmonicpercussive source separation, and background subtraction. We also present a network structure designed for paired input to make the most of the spatial information contained in the stereo. The experimental results show that the proposed network structures and the preprocessing methods effectively learn acoustic characteristics from the audio recordings, and their ensemble model significantly reduces the error rate further, exhibiting an accuracy of 0.917 for 4-fold cross-validation on the development. The proposed system achieved second place in DCASE 2017 task 1 with an accuracy of 0.804 on the evaluation set."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Jeong2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Audio Event Detection Using Multiple-Input Convolutional Neural Network</h4><a name="Jeong2017"></a>
<p>
                    Il-Young Jeong<sup>1,2</sup>, Subin Lee<sup>1,2</sup>, Yoonchang Han<sup>2</sup> and Kyogu Lee<sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Music and Audio Research Group, Seoul National University, Seoul, Korea, <sup>2</sup>Cochlear.ai, Seoul, Korea</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jeong_202.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Jeong_202_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseJeong2017c02e2ac48c054626b8c667f914ed8bb7" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseJeong2017c02e2ac48c054626b8c667f914ed8bb7" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingJeong2017c02e2ac48c054626b8c667f914ed8bb7" class="panel-collapse collapse" id="collapseJeong2017c02e2ac48c054626b8c667f914ed8bb7" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This paper describes the model and training framework from our submission for DCASE 2017 task 3: sound event detection in real life audio. Extending the basic convolutional neural network architecture, we use both short- and long-term audio signal simultaneously as input data. In the training stage, we calculated validation errors more frequently than one epoch with adaptive thresholds. We also used class-wise early-stopping strategy to find the best model for each class. The proposed model showed meaningful improvements in cross-validation experiments compared to the baseline system.</p>
<h5>Keywords</h5>
<p class="text-justify">DCASE 2017, Sound event detection, Convolutional neural networks</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexJeong2017c02e2ac48c054626b8c667f914ed8bb7" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jeong_202.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop_presentations/DCASE2017Workshop_Jeong_202_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexJeong2017c02e2ac48c054626b8c667f914ed8bb7label" class="modal fade" id="bibtexJeong2017c02e2ac48c054626b8c667f914ed8bb7" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexJeong2017c02e2ac48c054626b8c667f914ed8bb7label">Audio Event Detection Using Multiple-Input Convolutional Neural Network</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jeong2017,
    author = "Jeong, Il-Young and Lee, Subin and Han, Yoonchang and Lee, Kyogu",
    title = "Audio Event Detection Using Multiple-Input Convolutional Neural Network",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "51--54",
    keywords = "DCASE 2017, Sound event detection, Convolutional neural networks",
    abstract = "This paper describes the model and training framework from our submission for DCASE 2017 task 3: sound event detection in real life audio. Extending the basic convolutional neural network architecture, we use both short- and long-term audio signal simultaneously as input data. In the training stage, we calculated validation errors more frequently than one epoch with adaptive thresholds. We also used class-wise early-stopping strategy to find the best model for each class. The proposed model showed meaningful improvements in cross-validation experiments compared to the baseline system."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Jung2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>DNN-Based Audio Scene Classification for DCASE2017: Dual Input Features, Balancing Cost, and Stochastic Data Duplication</h4><a name="Jung2017"></a>
<p>
                    Jung Jee-Weon, Heo Hee-Soo, Yang IL-Ho, Yoon Sung-Hyun, Shim Hye-Jin and Yu Ha-Jin<br/>
<span class="text-muted"><small><em>School of Computer Science, University of Seoul, Seoul, Republic of South Korea</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jung_187.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Jung_187_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseJung2017e7a4c44dc62544a1b7c17295ddf48d32" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseJung2017e7a4c44dc62544a1b7c17295ddf48d32" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingJung2017e7a4c44dc62544a1b7c17295ddf48d32" class="panel-collapse collapse" id="collapseJung2017e7a4c44dc62544a1b7c17295ddf48d32" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">In this study, we explored DNN-based audio scene classification systems with dual input features. Dual input features take advantage of simultaneously utilizing two features with different levels of abstraction as inputs: a frame-level mel-filterbank feature and segment-level identity vector. A new fine-tune cost that solves the drawback of dual input features was developed, as well as a data duplication method that enables DNN to clearly discriminate frequently misclassified classes. Combining the proposed methods with the latest DNN techniques such as residual learning achieved a fold-wise accuracy of 95.9% for the validation set and 70.6% for the evaluation set provided by the Detection and Classification of Acoustic Scenes and Events community.</p>
<h5>Keywords</h5>
<p class="text-justify">audio scene classification, DNN, dual input feature, balancing cost, data duplication, residual learning</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexJung2017e7a4c44dc62544a1b7c17295ddf48d32" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Jung_187.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Jung_187_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexJung2017e7a4c44dc62544a1b7c17295ddf48d32label" class="modal fade" id="bibtexJung2017e7a4c44dc62544a1b7c17295ddf48d32" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexJung2017e7a4c44dc62544a1b7c17295ddf48d32label">DNN-Based Audio Scene Classification for DCASE2017: Dual Input Features, Balancing Cost, and Stochastic Data Duplication</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Jung2017,
    author = "Jung, Jee-Weon and Heo, Hee-Soo and Yang, IL-Ho and Yoon, Sung-Hyun and Shim, Hye-Jin and Yu, Ha-Jin",
    title = "{DNN}-Based Audio Scene Classification for {DCASE2017}: Dual Input Features, Balancing Cost, and Stochastic Data Duplication",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "59--63",
    keywords = "audio scene classification, DNN, dual input feature, balancing cost, data duplication, residual learning",
    abstract = "In this study, we explored DNN-based audio scene classification systems with dual input features. Dual input features take advantage of simultaneously utilizing two features with different levels of abstraction as inputs: a frame-level mel-filterbank feature and segment-level identity vector. A new fine-tune cost that solves the drawback of dual input features was developed, as well as a data duplication method that enables DNN to clearly discriminate frequently misclassified classes. Combining the proposed methods with the latest DNN techniques such as residual learning achieved a fold-wise accuracy of 95.9\% for the validation set and 70.6\% for the evaluation set provided by the Detection and Classification of Acoustic Scenes and Events community."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Lee2017a" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Combining Multi-Scale Features Using Sample-Level Deep Convolutional Neural Networks for Weakly Supervised Sound Event Detection</h4><a name="Lee2017a"></a>
<p>
                    Jongpil Lee<sup>1</sup>, Jiyoung Park<sup>1</sup>, Sangeun Kum<sup>1</sup>, Youngho Jeong<sup>2</sup>, Juhan Nam<sup>1</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Graduate School of Culture Technology, KAIST, Korea, <sup>2</sup>Realistic AV Research Group, ETRI, Korea</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lee_119.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Lee_119_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseLee2017acb5ba5640b9749d99747ef3ab2a1a68a" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseLee2017acb5ba5640b9749d99747ef3ab2a1a68a" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingLee2017acb5ba5640b9749d99747ef3ab2a1a68a" class="panel-collapse collapse" id="collapseLee2017acb5ba5640b9749d99747ef3ab2a1a68a" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This paper describes our method submitted to large-scale weakly supervised sound event detection for smart cars in the DCASE Challenge 2017. It is based on two deep neural network methods suggested for music auto-tagging. One is training sample-level Deep Convolutional Neural Networks (DCNN) using raw waveforms as a feature extractor. The other is aggregating features on multiscaled models of the DCNNs and making final predictions from them. With this approach, we achieved the best results, 47.3% in F-score on subtask A (audio tagging) and 0.75 in error rate on subtask B (sound event detection) in the evaluation. These results show that the waveform-based models can be comparable to spectrogrambased models when compared to other DCASE Task 4 submissions. Finally, we visualize hierarchically learned filters from the challenge dataset in each layer of the waveform-based model to explain how they discriminate the events.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound event detection, audio tagging, weakly supervised learning, multi-scale features, sample-level, convolutional neural networks, raw waveforms</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexLee2017acb5ba5640b9749d99747ef3ab2a1a68a" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Lee_119.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Lee_119_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexLee2017acb5ba5640b9749d99747ef3ab2a1a68alabel" class="modal fade" id="bibtexLee2017acb5ba5640b9749d99747ef3ab2a1a68a" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexLee2017acb5ba5640b9749d99747ef3ab2a1a68alabel">Combining Multi-Scale Features Using Sample-Level Deep Convolutional Neural Networks for Weakly Supervised Sound Event Detection</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Lee2017a,
    author = "Lee, Jongpil and Park, Jiyoung and Kum, Sangeun and Jeong, Youngho and Nam, Juhan",
    title = "Combining Multi-Scale Features Using Sample-Level Deep Convolutional Neural Networks for Weakly Supervised Sound Event Detection",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "69--73",
    keywords = "Sound event detection, audio tagging, weakly supervised learning, multi-scale features, sample-level, convolutional neural networks, raw waveforms",
    abstract = "This paper describes our method submitted to large-scale weakly supervised sound event detection for smart cars in the DCASE Challenge 2017. It is based on two deep neural network methods suggested for music auto-tagging. One is training sample-level Deep Convolutional Neural Networks (DCNN) using raw waveforms as a feature extractor. The other is aggregating features on multiscaled models of the DCNNs and making final predictions from them. With this approach, we achieved the best results, 47.3\% in F-score on subtask A (audio tagging) and 0.75 in error rate on subtask B (sound event detection) in the evaluation. These results show that the waveform-based models can be comparable to spectrogrambased models when compared to other DCASE Task 4 submissions. Finally, we visualize hierarchically learned filters from the challenge dataset in each layer of the waveform-based model to explain how they discriminate the events."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Vafeiadis2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning</h4><a name="Vafeiadis2017"></a>
<p>
                    Anastasios Vafeiadis<sup>1</sup>, Dimitrios Kalatzis<sup>1</sup>, Konstantinos Votis<sup>1</sup>, Dimitrios Giakoumis<sup>1</sup>, Dimitrios Tzovaras<sup>1</sup>, Liming Chen<sup>2</sup> and Raouf Hamzaoui<sup>2</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>Information Technologies Institute, Center for Research &amp; Technology Hellas, Thessaloniki, Greece, <sup>2</sup>Faculty of Technology, De Montfort University, Leicester, UK</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Vafeiadis_135.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Vafeiadis_135_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseVafeiadis20179f6d809723b24b85bf2b164b00273726" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseVafeiadis20179f6d809723b24b85bf2b164b00273726" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingVafeiadis20179f6d809723b24b85bf2b164b00273726" class="panel-collapse collapse" id="collapseVafeiadis20179f6d809723b24b85bf2b164b00273726" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This report describes our contribution to the 2017 Detection and Classification of Acoustic Scenes and Events (DCASE) challenge. We investigated two approaches for the acoustic scene classification task. Firstly, we used a combination of features in the time and frequency domain and a hybrid Support Vector Machines - Hidden Markov Model (SVM-HMM) classifier to achieve an average accuracy over 4-folds of 80.9% on the development dataset and 61.0% on the evaluation dataset. Secondly, by exploiting dataaugmentation techniques and using the whole segment (as opposed to splitting into sub-sequences) as an input, the accuracy of our CNN system was boosted to 95.9%. However, due to the small number of kernels used for the CNN and a failure of capturing the global information of the audio signals, it achieved an accuracy of 49.5% on the evaluation dataset. Our two approaches outperformed the DCASE baseline method, which uses log-mel band energies for feature extraction and a Multi-Layer Perceptron (MLP) to achieve an average accuracy over 4-folds of 74.8%.</p>
<h5>Keywords</h5>
<p class="text-justify">Acoustic scene classification, feature extraction, deep learning, spectral features, data augmentation</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexVafeiadis20179f6d809723b24b85bf2b164b00273726" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Vafeiadis_135.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017Workshop_Vafeiadis_135_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexVafeiadis20179f6d809723b24b85bf2b164b00273726label" class="modal fade" id="bibtexVafeiadis20179f6d809723b24b85bf2b164b00273726" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexVafeiadis20179f6d809723b24b85bf2b164b00273726label">Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Vafeiadis2017,
    author = "Vafeiadis, Anastasios and Kalatzis, Dimitrios and Votis, Konstantinos and Giakoumis, Dimitrios and Tzovaras, Dimitrios and Chen, Liming and Hamzaoui, Raouf",
    title = "Acoustic Scene Classification: From a Hybrid Classifier to Deep Learning",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "123--127",
    keywords = "Acoustic scene classification, feature extraction, deep learning, spectral features, data augmentation",
    abstract = "This report describes our contribution to the 2017 Detection and Classification of Acoustic Scenes and Events (DCASE) challenge. We investigated two approaches for the acoustic scene classification task. Firstly, we used a combination of features in the time and frequency domain and a hybrid Support Vector Machines - Hidden Markov Model (SVM-HMM) classifier to achieve an average accuracy over 4-folds of 80.9\% on the development dataset and 61.0\% on the evaluation dataset. Secondly, by exploiting dataaugmentation techniques and using the whole segment (as opposed to splitting into sub-sequences) as an input, the accuracy of our CNN system was boosted to 95.9\%. However, due to the small number of kernels used for the CNN and a failure of capturing the global information of the audio signals, it achieved an accuracy of 49.5\% on the evaluation dataset. Our two approaches outperformed the DCASE baseline method, which uses log-mel band energies for feature extraction and a Multi-Layer Perceptron (MLP) to achieve an average accuracy over 4-folds of 74.8\%."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Wang2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Audio Events Detection and classification using extended R-FCN Approach</h4><a name="Wang2017"></a>
<p>
                    Wang Kaiwu, Yang Liping and Yang Bin<br/>
<span class="text-muted"><small><em>Key Laboratory of Optoelectronic Technology and Systems (Chongqing University), Ministry of Education, ChongQing University, ChongQing, China</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Wang_121.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapseWang201773b1f0b002774d3fab8b9cadbbc3295d" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseWang201773b1f0b002774d3fab8b9cadbbc3295d" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingWang201773b1f0b002774d3fab8b9cadbbc3295d" class="panel-collapse collapse" id="collapseWang201773b1f0b002774d3fab8b9cadbbc3295d" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">In this study, we present a new audio event detection and classification approach based on R-FCN—a state-of-the-art fully convolutional network framework for visual object detection. Spectrogram features of audio signals are used as the input of the approach. The proposed approach consists of two stages like R-FCN network. In the first stage, we detect whether there are audio events by sliding convolutional kernel in time axis, and then proposals which possibly contain audio events are generated by RPN (Region Proposal Networks). In the second stage, time and frequency domain information are integrated to classify these proposals and refine their boundaries. Our approach can output the positions of audio events directly which can input a two-dimensional representation of arbitrary length sound without any size regularization.</p>
<h5>Keywords</h5>
<p class="text-justify">audio events detection, Convolutional Neural Network, spectrogram feature</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexWang201773b1f0b002774d3fab8b9cadbbc3295d" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Wang_121.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexWang201773b1f0b002774d3fab8b9cadbbc3295dlabel" class="modal fade" id="bibtexWang201773b1f0b002774d3fab8b9cadbbc3295d" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexWang201773b1f0b002774d3fab8b9cadbbc3295dlabel">Audio Events Detection and classification using extended R-FCN Approach</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Wang2017,
    author = "Wang, Kaiwu and Yang, Liping and Yang, Bin",
    title = "Audio Events Detection and classification using extended {R-FCN} Approach",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "128--132",
    keywords = "audio events detection, Convolutional Neural Network, spectrogram feature",
    abstract = "In this study, we present a new audio event detection and classification approach based on R-FCN—a state-of-the-art fully convolutional network framework for visual object detection. Spectrogram features of audio signals are used as the input of the approach. The proposed approach consists of two stages like R-FCN network. In the first stage, we detect whether there are audio events by sliding convolutional kernel in time axis, and then proposals which possibly contain audio events are generated by RPN (Region Proposal Networks). In the second stage, time and frequency domain information are integrated to classify these proposals and refine their boundaries. Our approach can output the positions of audio events directly which can input a two-dimensional representation of arbitrary length sound without any size regularization."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Zheng2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion</h4><a name="Zheng2017"></a>
<p>
                    Zheng Weiping<sup>1</sup>, Yi Jiantao<sup>1</sup>, Xing Xiaotao<sup>1</sup>, Liu Xiangtao<sup>2</sup> and Peng Shaohu<sup>3</sup><br/>
<span class="text-muted"><small><em><sup>1</sup>School of Computer, South China Normal University, Guangzhou, China, <sup>2</sup>Shenzhen Chinasfan Information Technology Co., Ltd., Shenzhen Chinasfan Information Technology Co., Ltd., Shenzhen, China, <sup>3</sup>School of Mechanical and Electrical Engineering,, Guangzhou University, Guangzhou, China</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Zheng_159.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-xs btn-info btn-btex" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017_Xing_158_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o fa-1x"></i></a>
<button aria-controls="collapseZheng2017f16fc9b2b8a446c798a95253d0d2b8b9" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseZheng2017f16fc9b2b8a446c798a95253d0d2b8b9" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingZheng2017f16fc9b2b8a446c798a95253d0d2b8b9" class="panel-collapse collapse" id="collapseZheng2017f16fc9b2b8a446c798a95253d0d2b8b9" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">Making sense of the environment by sounds is an important research in machine learning community. In this work, a Deep Convolutional Neural Network (DCNN) model is presented to classify acoustic scenes along with a multiple spectrograms fusion method. Firstly, the generations of standard spectrogram and CQT spectrogram are introduced separately. Corresponding features can then be extracted by feeding these spectrogram data into the proposed DCNN model. To fuse these multiple spectrogram features, two fusing mechanisms, namely the voting and the SVM methods, are designed. By fusing DCNN features of the standard and CQT spectrograms, the accuracy is significantly improved in our experiments, comparing with the single spectrogram schemes. This proves the effectiveness of the proposed multi-spectrograms fusion method.</p>
<h5>Keywords</h5>
<p class="text-justify">Deep convolutional neural network, spectrogram, feature fusion, acoustic scene classification</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexZheng2017f16fc9b2b8a446c798a95253d0d2b8b9" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Zheng_159.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<a class="btn btn-sm btn-info btn-btex2" data-placement="bottom" href="../documents/workshop2017/presentations/DCASE2017_Xing_158_poster.pdf" rel="tooltip" title="Download poster"><i class="fa fa-picture-o"></i> Poster</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexZheng2017f16fc9b2b8a446c798a95253d0d2b8b9label" class="modal fade" id="bibtexZheng2017f16fc9b2b8a446c798a95253d0d2b8b9" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexZheng2017f16fc9b2b8a446c798a95253d0d2b8b9label">Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Zheng2017,
    author = "Zheng, Weiping and Yi, Jiantao and Xing, Xiaotao and Liu, Xiangtao and Peng, Shaohu",
    title = "Acoustic Scene Classification Using Deep Convolutional Neural Network and Multiple Spectrograms Fusion",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "133--137",
    keywords = "Deep convolutional neural network, spectrogram, feature fusion, acoustic scene classification",
    abstract = "Making sense of the environment by sounds is an important research in machine learning community. In this work, a Deep Convolutional Neural Network (DCNN) model is presented to classify acoustic scenes along with a multiple spectrograms fusion method. Firstly, the generations of standard spectrogram and CQT spectrogram are introduced separately. Corresponding features can then be extracted by feeding these spectrogram data into the proposed DCNN model. To fuse these multiple spectrogram features, two fusing mechanisms, namely the voting and the SVM methods, are designed. By fusing DCNN features of the standard and CQT spectrograms, the accuracy is significantly improved in our experiments, comparing with the single spectrogram schemes. This proves the effectiveness of the proposed multi-spectrograms fusion method."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Zhou2017" data-source="content/data/workshop2017/proceedings.bib" data-template="fancy_minimal">
<div class="row">
<div class="col-md-9">
<h4>Robust Sound Event Detection Through Noise Estimation and Source Separation Using NMF</h4><a name="Zhou2017"></a>
<p>
                    Qing Zhou and Zuren Feng<br/>
<span class="text-muted"><small><em>School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi'an, China</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Zhou_113.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapseZhou2017f0051a3da98945419a238aa38744b036" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseZhou2017f0051a3da98945419a238aa38744b036" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingZhou2017f0051a3da98945419a238aa38744b036" class="panel-collapse collapse" id="collapseZhou2017f0051a3da98945419a238aa38744b036" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">This paper addresses the problem of sound event detection under non-stationary noises and various real-world acoustic scenes. An effective noise reduction strategy is proposed in this paper which can automatically adapt to background variations. The proposed method is based on supervised non-negative matrix factorization (NMF) for separating target events from noise. The event dictionary is trained offline using the training data of the target event class while the noise dictionary is learned online from the input signal by sparse and low-rank decomposition. Incorporating the estimated noise bases, this method can produce accurate source separation results by reducing noise residue and signal distortion of the reconstructed event spectrogram. Experimental results on DCASE 2017 task 2 dataset show that the proposed method outperforms the baseline system based on multi-layer perceptron classifiers and also another NMF-based method which employs a semi-supervised strategy for noise reduction.</p>
<h5>Keywords</h5>
<p class="text-justify">Sound event detection, non-negative matrix factorization, sparse and low-rank decomposition, source separation</p>
<div class="btn-group">
<button class="btn btn-sm btn-danger" data-target="#bibtexZhou2017f0051a3da98945419a238aa38744b036" data-toggle="modal" type="button"><i class="fa fa-file-text-o"></i> Bibtex</button>
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/workshop2017/proceedings/DCASE2017Workshop_Zhou_113.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
<!-- Modal -->
<div aria-hidden="true" aria-labelledby="bibtexZhou2017f0051a3da98945419a238aa38744b036label" class="modal fade" id="bibtexZhou2017f0051a3da98945419a238aa38744b036" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true" class="glyphicon glyphicon-remove-sign"></span><span class="sr-only">Close</span></button>
<h4 class="modal-title" id="bibtexZhou2017f0051a3da98945419a238aa38744b036label">Robust Sound Event Detection Through Noise Estimation and Source Separation Using NMF</h4>
</div>
<div class="modal-body">
<pre>@inproceedings{Zhou2017,
    author = "Zhou, Qing and Feng, Zuren",
    title = "Robust Sound Event Detection Through Noise Estimation and Source Separation Using {NMF}",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2017 Workshop (DCASE2017)",
    year = "2017",
    month = "November",
    pages = "138--142",
    keywords = "Sound event detection, non-negative matrix factorization, sparse and low-rank decomposition, source separation",
    abstract = "This paper addresses the problem of sound event detection under non-stationary noises and various real-world acoustic scenes. An effective noise reduction strategy is proposed in this paper which can automatically adapt to background variations. The proposed method is based on supervised non-negative matrix factorization (NMF) for separating target events from noise. The event dictionary is trained offline using the training data of the target event class while the noise dictionary is learned online from the input signal by sparse and low-rank decomposition. Incorporating the estimated noise bases, this method can produce accurate source separation results by reducing noise residue and signal distortion of the reconstructed event spectrogram. Experimental results on DCASE 2017 task 2 dataset show that the proposed method outperforms the baseline system based on multi-layer perceptron classifiers and also another NMF-based method which employs a semi-supervised strategy for noise reduction."
}
</pre>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal" type="button">Close</button>
</div>
</div>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="border-top: none;padding-top:10px;"></td>
<td style="border-top: none;padding-top:0px;">
<div class="btex-item" data-item="Vesperini2017" data-source="content/data/challenge2017/technical_reports_task2.bib" data-template="fancy_minimal_no_bibtex">
<div class="row">
<div class="col-md-9">
<h4>A Hierarchic Multi-Scaled Approach for Rare Sound Event Detection</h4><a name="Vesperini2017"></a>
<p>
                    Fabio Vesperini, Diego Droghini, Daniele Ferretti, Emanuele Principi, Leonardo Gabrielli, Stefano Squartini and Francesco Piazza<br/>
<span class="text-muted"><small><em>Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy</em></small></span>
</p>
</div>
<div class="col-md-3">
<div class="btn-group pull-right">
<a class="btn btn-xs btn-warning btn-btex" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Vesperini_216.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
<button aria-controls="collapseVesperini20177947beb259b74443aec9d21a7b865625" aria-expanded="true" class="btn btn-default btn-xs" data-parent="#btex-items-accordion" data-toggle="collapse" href="#collapseVesperini20177947beb259b74443aec9d21a7b865625" type="button">
<i class="fa fa-caret-down"></i>
</button>
</div>
</div>
</div>
<div aria-labelledby="headingVesperini20177947beb259b74443aec9d21a7b865625" class="panel-collapse collapse" id="collapseVesperini20177947beb259b74443aec9d21a7b865625" role="tabpanel">
<h5>Abstract</h5>
<p class="text-justify">We propose a system for rare sound event detection using hierarchical and multi-scaled approach based on Multi Layer Perceptron (MLP) and Convolutional Neural Networks (CNN). It is our contribution to the rare sound event detection task of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE2017). The task consists on detection of event onset from artificially generated mixtures. Acoustic features are extracted from the acoustic signals, successively first event detection stage is performed by an MLP based neural network which proposes contiguous blocks of frames to the second stage. The CNN refines the event detection of the prior network, intrinsically operating on a multi-scaled resolution and discarding blocks that contain background wrongly classified by the MLP as event. Finally the effective onset time of the active event is obtained. The achieved overall error rate and F-measure on the development testset are respectively equal to 0.18 and 90.9%.</p>
<div class="btn-group">
<a class="btn btn-sm btn-warning btn-btex2" data-placement="bottom" href="../documents/challenge2017/technical_reports/DCASE2017_Vesperini_216.pdf" rel="tooltip" title="Download pdf"><i class="fa fa-file-text fa-1x"></i> PDF</a>
</div>
<div class="btn-group">
</div>
</div>
</div>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>12:00</td>
<td class="danger">Closing remarks</td>
<td>
<table class="table" style="height:100%; border-bottom: none;padding-top:0px;">
<tr>
<td style="border-top: none;padding-top:0px;width:80px;"></td>
<td style="border-top: none;padding-top:0px;"><h3 id="closing-remarks">Closing remarks</h3></td>
</tr>
</table>
</td>
</tr>
</tbodv></table>
<script type="text/javascript">
  $('.collapse').on('show.bs.collapse', function () {
    $('.collapse.in').each(function(){
        $(this).collapse('hide');
    });
  });
</script>
                </div>
            </section>
        </div>
    </div>
</div>    
<br><br>
<ul class="nav pull-right scroll-top">
  <li>
    <a title="Scroll to top" href="#" class="page-scroll-top">
      <i class="glyphicon glyphicon-chevron-up"></i>
    </a>
  </li>
</ul><script type="text/javascript" src="https://biodcase.github.io/theme/assets/jquery/jquery.easing.min.js"></script>
<script type="text/javascript" src="https://biodcase.github.io/theme/assets/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://biodcase.github.io/theme/assets/scrollreveal/scrollreveal.min.js"></script>
<script type="text/javascript" src="https://biodcase.github.io/theme/js/setup.scrollreveal.js"></script>
<script type="text/javascript" src="https://biodcase.github.io/theme/assets/highlight/highlight.pack.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre code:not([class])').forEach(function($) {$.className = 'no-highlight';});
    hljs.initHighlightingOnLoad();
</script>
<script type="text/javascript" src="https://biodcase.github.io/theme/js/respond.min.js"></script>
<script type="text/javascript" src="https://biodcase.github.io/theme/js/btoc.min.js"></script>
<script type="text/javascript" src="https://biodcase.github.io/theme/js/theme.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114253890-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114253890-1');
</script>
</body>
</html>